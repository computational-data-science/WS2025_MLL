{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db477f7b-17fd-4ee2-8538-aab6b839850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429f2c7-a2aa-4923-966e-7048ff3367fe",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c938ff34-182b-4aac-b8af-8cfd777cf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStumpDebug:\n",
    "    \"\"\"\n",
    "    Ein Decision Stump mit vollständiger Console-Ausgabe.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.polarity = None\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        X = X.flatten()\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(len(y)) / len(y)\n",
    "\n",
    "        best_err = 999999\n",
    "        best_thresh = None\n",
    "        best_polarity = None\n",
    "\n",
    "        print(\"\\n=== Decision Stump Training ===\")\n",
    "        print(\"X:\", X)\n",
    "        print(\"y:\", y)\n",
    "        print(\"w:\", sample_weight)\n",
    "\n",
    "        for thresh in np.unique(X):\n",
    "            for polarity in [+1, -1]:\n",
    "                preds = np.ones(len(y))\n",
    "                preds[X * polarity < thresh * polarity] = -1\n",
    "\n",
    "                err = np.sum(sample_weight[preds != y])\n",
    "\n",
    "                print(f\"  Teste thresh={thresh:.3f}, pol={polarity}, \"\n",
    "                      f\"pred={preds}, error={err:.4f}\")\n",
    "\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    best_thresh = thresh\n",
    "                    best_polarity = polarity\n",
    "\n",
    "        print(\"\\n--> Gewählter Stump:\")\n",
    "        print(\"Threshold:\", best_thresh)\n",
    "        print(\"Polarity:\", best_polarity)\n",
    "        print(\"Error:\", best_err)\n",
    "        print(\"===============================\\n\")\n",
    "\n",
    "        self.threshold = best_thresh\n",
    "        self.polarity = best_polarity\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.flatten()\n",
    "        preds = np.ones(len(X))\n",
    "        preds[X * self.polarity < self.threshold * self.polarity] = -1\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03598334-f4af-478c-b75e-5212122456d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostDebug:\n",
    "    def __init__(self, T=5):\n",
    "        self.T = T\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(y)\n",
    "        w = np.ones(n) / n\n",
    "        \n",
    "        print(\"\\n================ AdaBoost Training ================\\n\")\n",
    "        \n",
    "        for t in range(1, self.T+1):\n",
    "            print(f\"\\n=== Iteration {t} ===\")\n",
    "            print(\"Aktuelle Gewichte:\", np.round(w, 4))\n",
    "\n",
    "            stump = DecisionStumpDebug()\n",
    "            stump.fit(X, y, sample_weight=w)\n",
    "            preds = stump.predict(X)\n",
    "\n",
    "            err = np.sum(w[preds != y])\n",
    "            err = max(err, 1e-10)\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "            print(f\"Fehler err_t = {err:.6f}\")\n",
    "            print(f\"Alpha_t = {alpha:.6f}\")\n",
    "            print(\"Predictions:\", preds)\n",
    "\n",
    "            # Gewichte updaten\n",
    "            w = w * np.exp(-alpha * y * preds)\n",
    "            print(\"Un-normalisierte neue Gewichte:\", np.round(w, 6))\n",
    "\n",
    "            w = w / np.sum(w)\n",
    "            print(\"Normalisierte neue Gewichte:\", np.round(w, 6))\n",
    "\n",
    "            self.models.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "        print(\"\\n================ Training fertig ================\\n\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(len(X))\n",
    "        for alpha, stump in zip(self.alphas, self.models):\n",
    "            preds += alpha * stump.predict(X)\n",
    "        print(\"Summenvorhersage F(x):\", preds)\n",
    "        return np.sign(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3237f36d-86e0-4c4f-b543-a4ecaa2c248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ AdaBoost Training ================\n",
      "\n",
      "\n",
      "=== Iteration 1 ===\n",
      "Aktuelle Gewichte: [0.1667 0.1667 0.1667 0.1667 0.1667 0.1667]\n",
      "\n",
      "=== Decision Stump Training ===\n",
      "X: [1. 2. 3. 4. 5. 6.]\n",
      "y: [-1 -1 -1  1 -1  1]\n",
      "w: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "  Teste thresh=1.000, pol=1, pred=[1. 1. 1. 1. 1. 1.], error=0.6667\n",
      "  Teste thresh=1.000, pol=-1, pred=[ 1. -1. -1. -1. -1. -1.], error=0.5000\n",
      "  Teste thresh=2.000, pol=1, pred=[-1.  1.  1.  1.  1.  1.], error=0.5000\n",
      "  Teste thresh=2.000, pol=-1, pred=[ 1.  1. -1. -1. -1. -1.], error=0.6667\n",
      "  Teste thresh=3.000, pol=1, pred=[-1. -1.  1.  1.  1.  1.], error=0.3333\n",
      "  Teste thresh=3.000, pol=-1, pred=[ 1.  1.  1. -1. -1. -1.], error=0.8333\n",
      "  Teste thresh=4.000, pol=1, pred=[-1. -1. -1.  1.  1.  1.], error=0.1667\n",
      "  Teste thresh=4.000, pol=-1, pred=[ 1.  1.  1.  1. -1. -1.], error=0.6667\n",
      "  Teste thresh=5.000, pol=1, pred=[-1. -1. -1. -1.  1.  1.], error=0.3333\n",
      "  Teste thresh=5.000, pol=-1, pred=[ 1.  1.  1.  1.  1. -1.], error=0.8333\n",
      "  Teste thresh=6.000, pol=1, pred=[-1. -1. -1. -1. -1.  1.], error=0.1667\n",
      "  Teste thresh=6.000, pol=-1, pred=[1. 1. 1. 1. 1. 1.], error=0.6667\n",
      "\n",
      "--> Gewählter Stump:\n",
      "Threshold: 4.0\n",
      "Polarity: 1\n",
      "Error: 0.16666666666666666\n",
      "===============================\n",
      "\n",
      "Fehler err_t = 0.166667\n",
      "Alpha_t = 0.804719\n",
      "Predictions: [-1. -1. -1.  1.  1.  1.]\n",
      "Un-normalisierte neue Gewichte: [0.074536 0.074536 0.074536 0.074536 0.372678 0.074536]\n",
      "Normalisierte neue Gewichte: [0.1 0.1 0.1 0.1 0.5 0.1]\n",
      "\n",
      "=== Iteration 2 ===\n",
      "Aktuelle Gewichte: [0.1 0.1 0.1 0.1 0.5 0.1]\n",
      "\n",
      "=== Decision Stump Training ===\n",
      "X: [1. 2. 3. 4. 5. 6.]\n",
      "y: [-1 -1 -1  1 -1  1]\n",
      "w: [0.1 0.1 0.1 0.1 0.5 0.1]\n",
      "  Teste thresh=1.000, pol=1, pred=[1. 1. 1. 1. 1. 1.], error=0.8000\n",
      "  Teste thresh=1.000, pol=-1, pred=[ 1. -1. -1. -1. -1. -1.], error=0.3000\n",
      "  Teste thresh=2.000, pol=1, pred=[-1.  1.  1.  1.  1.  1.], error=0.7000\n",
      "  Teste thresh=2.000, pol=-1, pred=[ 1.  1. -1. -1. -1. -1.], error=0.4000\n",
      "  Teste thresh=3.000, pol=1, pred=[-1. -1.  1.  1.  1.  1.], error=0.6000\n",
      "  Teste thresh=3.000, pol=-1, pred=[ 1.  1.  1. -1. -1. -1.], error=0.5000\n",
      "  Teste thresh=4.000, pol=1, pred=[-1. -1. -1.  1.  1.  1.], error=0.5000\n",
      "  Teste thresh=4.000, pol=-1, pred=[ 1.  1.  1.  1. -1. -1.], error=0.4000\n",
      "  Teste thresh=5.000, pol=1, pred=[-1. -1. -1. -1.  1.  1.], error=0.6000\n",
      "  Teste thresh=5.000, pol=-1, pred=[ 1.  1.  1.  1.  1. -1.], error=0.9000\n",
      "  Teste thresh=6.000, pol=1, pred=[-1. -1. -1. -1. -1.  1.], error=0.1000\n",
      "  Teste thresh=6.000, pol=-1, pred=[1. 1. 1. 1. 1. 1.], error=0.8000\n",
      "\n",
      "--> Gewählter Stump:\n",
      "Threshold: 6.0\n",
      "Polarity: 1\n",
      "Error: 0.1\n",
      "===============================\n",
      "\n",
      "Fehler err_t = 0.100000\n",
      "Alpha_t = 1.098612\n",
      "Predictions: [-1. -1. -1. -1. -1.  1.]\n",
      "Un-normalisierte neue Gewichte: [0.033333 0.033333 0.033333 0.3      0.166667 0.033333]\n",
      "Normalisierte neue Gewichte: [0.055556 0.055556 0.055556 0.5      0.277778 0.055556]\n",
      "\n",
      "=== Iteration 3 ===\n",
      "Aktuelle Gewichte: [0.0556 0.0556 0.0556 0.5    0.2778 0.0556]\n",
      "\n",
      "=== Decision Stump Training ===\n",
      "X: [1. 2. 3. 4. 5. 6.]\n",
      "y: [-1 -1 -1  1 -1  1]\n",
      "w: [0.05555556 0.05555556 0.05555556 0.5        0.27777778 0.05555556]\n",
      "  Teste thresh=1.000, pol=1, pred=[1. 1. 1. 1. 1. 1.], error=0.4444\n",
      "  Teste thresh=1.000, pol=-1, pred=[ 1. -1. -1. -1. -1. -1.], error=0.6111\n",
      "  Teste thresh=2.000, pol=1, pred=[-1.  1.  1.  1.  1.  1.], error=0.3889\n",
      "  Teste thresh=2.000, pol=-1, pred=[ 1.  1. -1. -1. -1. -1.], error=0.6667\n",
      "  Teste thresh=3.000, pol=1, pred=[-1. -1.  1.  1.  1.  1.], error=0.3333\n",
      "  Teste thresh=3.000, pol=-1, pred=[ 1.  1.  1. -1. -1. -1.], error=0.7222\n",
      "  Teste thresh=4.000, pol=1, pred=[-1. -1. -1.  1.  1.  1.], error=0.2778\n",
      "  Teste thresh=4.000, pol=-1, pred=[ 1.  1.  1.  1. -1. -1.], error=0.2222\n",
      "  Teste thresh=5.000, pol=1, pred=[-1. -1. -1. -1.  1.  1.], error=0.7778\n",
      "  Teste thresh=5.000, pol=-1, pred=[ 1.  1.  1.  1.  1. -1.], error=0.5000\n",
      "  Teste thresh=6.000, pol=1, pred=[-1. -1. -1. -1. -1.  1.], error=0.5000\n",
      "  Teste thresh=6.000, pol=-1, pred=[1. 1. 1. 1. 1. 1.], error=0.4444\n",
      "\n",
      "--> Gewählter Stump:\n",
      "Threshold: 4.0\n",
      "Polarity: -1\n",
      "Error: 0.22222222222222224\n",
      "===============================\n",
      "\n",
      "Fehler err_t = 0.222222\n",
      "Alpha_t = 0.626381\n",
      "Predictions: [ 1.  1.  1.  1. -1. -1.]\n",
      "Un-normalisierte neue Gewichte: [0.103935 0.103935 0.103935 0.267261 0.148478 0.103935]\n",
      "Normalisierte neue Gewichte: [0.125    0.125    0.125    0.321429 0.178571 0.125   ]\n",
      "\n",
      "================ Training fertig ================\n",
      "\n",
      "== Finale Vorhersage ==\n",
      "Summenvorhersage F(x): [-1.27694976 -1.27694976 -1.27694976  0.33248815 -0.92027482  1.27694976]\n",
      "[-1. -1. -1.  1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[2],[3],[4],[5],[6]], dtype=float)\n",
    "y = np.array([-1,-1,-1,+1,-1,+1], dtype=int)\n",
    "\n",
    "clf = AdaBoostDebug(T=3)\n",
    "clf.fit(X, y)\n",
    "print(\"== Finale Vorhersage ==\")\n",
    "print(clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b6b8f-593a-4f3b-9d6a-51743e6cd96c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc04cc6-898c-4ac2-9f7f-645562377e77",
   "metadata": {},
   "source": [
    "# AdaBoost und Gradient Boosting: Eine vergleichende Erklärung\n",
    "\n",
    "Sowohl AdaBoost (Adaptive Boosting) als auch Gradient Boosting (GBM) gehören zu den **Boosting-Algorithmen**, die schwache Lerner (meist Entscheidungsbäume) sequenziell kombinieren, um ein starkes Gesamtmodell zu erzeugen. Der Hauptunterschied liegt jedoch in der Methode, wie sie die Fehler des Vorgängermodells korrigieren.\n",
    "\n",
    "## 1. AdaBoost: Fokus auf Datenpunktgewichten\n",
    "\n",
    "AdaBoost wurde entwickelt, bevor das allgemeine Gradient-Boosting-Framework existierte.\n",
    "\n",
    "* **Mechanismus:** AdaBoost passt die **Gewichtung** der Trainingsdatenpunkte in jeder Iteration an. Falsch klassifizierte Punkte erhalten höhere Gewichte, wodurch der nächste schwache Lerner gezwungen wird, sich auf diese \"schwierigen\" Beispiele zu konzentrieren.\n",
    "* **Verlustfunktion:** AdaBoost ist **mathematisch äquivalent** zur schrittweisen Minimierung der **exponentiellen Verlustfunktion** $\\mathcal{L}_{\\text{exp}}(y, F) = e^{-yF}$.\n",
    "* **Fazit:** AdaBoost ist **kein** Gradient Boosting im allgemeinen Sinne, aber ein **Spezialfall** des Additiven Modells.\n",
    "\n",
    "\n",
    "## 2. Gradient Boosting (GBM): Fokus auf Gradienten\n",
    "\n",
    "GBM ist ein verallgemeinertes Framework, das jede differenzierbare Verlustfunktion verwenden kann.\n",
    "\n",
    "### Das Prinzip der Pseudo-Residuen\n",
    "\n",
    "Der nächste schwache Lerner wird nicht auf die Originaldaten oder ihre Gewichte trainiert, sondern auf die **Pseudo-Residuen** ($g_i$):\n",
    "\n",
    "$$g_i = -\\left[\\frac{\\partial \\mathcal{L}(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}\\right]$$\n",
    "\n",
    "Der negative Gradient ($\\mathbf{-} \\nabla \\mathcal{L}$) zeigt geometrisch die Richtung des **steilsten Abstiegs** der Verlustfunktion. Das Pseudo-Residuum ist die **optimale Korrektur** (das Residuum), die der nächste Lerner finden muss, um den Gesamtfehler am schnellsten zu verringern.\n",
    "\n",
    "| Merkmal | **AdaBoost** | **Gradient Boosting (GBM)** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Fehlerkorrektur** | Anpassung der **Datenpunktgewichte** | Training auf **Negativem Gradienten** (Pseudo-Residuen) |\n",
    "| **Verlustfunktion** | Exponentiell (fix) | Beliebig (wählbar) |\n",
    "| **Sonderfall** | GBM mit exponentieller Verlustfunktion | Allgemeiner Rahmen |\n",
    "\n",
    "### Die Äquivalenz zum Residuum (Quadrierter Fehler)\n",
    "\n",
    "Bei der Verwendung des **Quadrierten Fehlers** (Mean Squared Error, MSE) $\\mathcal{L}(y, F) = \\frac{1}{2}(y - F)^2$ vereinfacht sich der negative Gradient genau zum klassischen Residuum:\n",
    "\n",
    "$$g_i = - \\frac{\\partial \\mathcal{L}}{\\partial F} = y_i - F_{m-1}(x_i)$$\n",
    "\n",
    "Daher **entspricht** das Pseudo-Residuum in diesem speziellen Fall dem tatsächlichen Residuum.\n",
    "\n",
    "\n",
    "## Wahl des Schwachen Lerners\n",
    "\n",
    "* Gradient Boosting verwendet typischerweise **Regressionsbäume** (Regression Trees) als schwache Lerner.\n",
    "* **Grund:** Die Pseudo-Residuen $g_i$ sind **kontinuierliche (reelle) Werte**, selbst bei Klassifikationsproblemen (dort sind es die Fehler auf der Logit-Skala). Regressionsbäume sind darauf ausgelegt, kontinuierliche Zielwerte optimal vorherzusagen, im Gegensatz zu Klassifikationsbäumen, die diskrete Klassen-Labels vorhersagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf76723-8548-4199-b2eb-33ca6e123d6c",
   "metadata": {},
   "source": [
    "# GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b9e3342-5b3a-46eb-87de-89ac30c55c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionStumpDebug:\n",
    "    \"\"\"\n",
    "    Minimaler Regressions-Stump für Gradient Boosting:\n",
    "    - 1 Feature\n",
    "    - threshold + konstante Werte (c_left, c_right)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.polarity = 1\n",
    "        self.c_left = None\n",
    "        self.c_right = None\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        X = X.flatten()\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(len(y)) / len(y)\n",
    "\n",
    "        best_err = np.inf\n",
    "\n",
    "        print(\"\\n=== Regression Stump Training ===\")\n",
    "        print(\"X:\", X)\n",
    "        print(\"Zielwerte (Pseudo-Residuen):\", np.round(y, 4))\n",
    "        print(\"Gewichte:\", np.round(sample_weight, 4))\n",
    "\n",
    "        for thresh in np.unique(X):\n",
    "            for polarity in [+1, -1]:\n",
    "\n",
    "                left = (X * polarity) < (thresh * polarity)\n",
    "                right = ~left\n",
    "\n",
    "                if left.sum() == 0 or right.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                w_left = sample_weight[left]\n",
    "                w_right = sample_weight[right]\n",
    "\n",
    "                c_left = np.sum(w_left * y[left]) / np.sum(w_left)\n",
    "                c_right = np.sum(w_right * y[right]) / np.sum(w_right)\n",
    "\n",
    "                preds = np.where(left, c_left, c_right)\n",
    "                err = np.sum(sample_weight * (y - preds)**2)\n",
    "\n",
    "                print(f\"  thresh={thresh}, pol={polarity}, \"\n",
    "                      f\"c_left={c_left:.3f}, c_right={c_right:.3f}, \"\n",
    "                      f\"err={err:.5f}\")\n",
    "\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    self.threshold = thresh\n",
    "                    self.polarity = polarity\n",
    "                    self.c_left = c_left\n",
    "                    self.c_right = c_right\n",
    "\n",
    "        print(\"\\n--> Gewählter Regressions-Stump:\")\n",
    "        print(\"Threshold:\", self.threshold)\n",
    "        print(\"Polarity:\", self.polarity)\n",
    "        print(\"c_left:\", self.c_left)\n",
    "        print(\"c_right:\", self.c_right)\n",
    "        print(\"Minimaler Fehler:\", best_err)\n",
    "        print(\"=================================\\n\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.flatten()\n",
    "        left = (X * self.polarity) < (self.threshold * self.polarity)\n",
    "        return np.where(left, self.c_left, self.c_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738262ed-9e87-4fd9-8662-412c3400c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b2d5e7f-61e4-46e0-9440-6fc53e7537b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticGradientBoostingDebug:\n",
    "\n",
    "    def __init__(self, T=5, lr=0.5):\n",
    "        self.T = T\n",
    "        self.lr = lr\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "        # Start mit log-odds der Grundwahrscheinlichkeit\n",
    "        p0 = np.mean(y)\n",
    "        self.F0 = np.log(p0 / (1 - p0))\n",
    "        F = np.ones(len(y)) * self.F0\n",
    "\n",
    "        print(\"\\n========== Logistic Gradient Boosting Training ==========\")\n",
    "        print(f\"Startwert F0 (log-odds) = {self.F0:.4f}\")\n",
    "\n",
    "        for t in range(1, self.T+1):\n",
    "            print(f\"\\n=== Iteration {t} ===\")\n",
    "\n",
    "            p = sigmoid(F)\n",
    "            r = y - p  # Pseudo-Residuals\n",
    "\n",
    "            print(\"Aktuelle Logits F:\", np.round(F, 4))\n",
    "            print(\"Aktuelle Wahrsch. p:\", np.round(p, 4))\n",
    "            print(\"Pseudo-Residuen r:\", np.round(r, 4))\n",
    "\n",
    "            stump = RegressionStumpDebug()\n",
    "            stump.fit(X, r)\n",
    "\n",
    "            pred = stump.predict(X)\n",
    "            print(\"Stump Predictions h_t(x):\", np.round(pred, 4))\n",
    "\n",
    "            F = F + self.lr * pred\n",
    "            print(\"Neue Logits F:\", np.round(F, 4))\n",
    "\n",
    "            self.models.append(stump)\n",
    "\n",
    "        print(\"\\n========= Training beendet =========\\n\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        F = np.ones(len(X)) * self.F0\n",
    "        for stump in self.models:\n",
    "            F += self.lr * stump.predict(X)\n",
    "        return sigmoid(F)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d829cca-b4d5-472d-b6c4-6500a1dee3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Logistic Gradient Boosting Training ==========\n",
      "Startwert F0 (log-odds) = -0.6931\n",
      "\n",
      "=== Iteration 1 ===\n",
      "Aktuelle Logits F: [-0.6931 -0.6931 -0.6931 -0.6931 -0.6931 -0.6931]\n",
      "Aktuelle Wahrsch. p: [0.3333 0.3333 0.3333 0.3333 0.3333 0.3333]\n",
      "Pseudo-Residuen r: [-0.3333 -0.3333 -0.3333  0.6667 -0.3333  0.6667]\n",
      "\n",
      "=== Regression Stump Training ===\n",
      "X: [1. 2. 3. 4. 5. 6.]\n",
      "Zielwerte (Pseudo-Residuen): [-0.3333 -0.3333 -0.3333  0.6667 -0.3333  0.6667]\n",
      "Gewichte: [0.1667 0.1667 0.1667 0.1667 0.1667 0.1667]\n",
      "  thresh=1.0, pol=-1, c_left=0.067, c_right=-0.333, err=0.20000\n",
      "  thresh=2.0, pol=1, c_left=-0.333, c_right=0.067, err=0.20000\n",
      "  thresh=2.0, pol=-1, c_left=0.167, c_right=-0.333, err=0.16667\n",
      "  thresh=3.0, pol=1, c_left=-0.333, c_right=0.167, err=0.16667\n",
      "  thresh=3.0, pol=-1, c_left=0.333, c_right=-0.333, err=0.11111\n",
      "  thresh=4.0, pol=1, c_left=-0.333, c_right=0.333, err=0.11111\n",
      "  thresh=4.0, pol=-1, c_left=0.167, c_right=-0.083, err=0.20833\n",
      "  thresh=5.0, pol=1, c_left=-0.083, c_right=0.167, err=0.20833\n",
      "  thresh=5.0, pol=-1, c_left=0.667, c_right=-0.133, err=0.13333\n",
      "  thresh=6.0, pol=1, c_left=-0.133, c_right=0.667, err=0.13333\n",
      "\n",
      "--> Gewählter Regressions-Stump:\n",
      "Threshold: 3.0\n",
      "Polarity: -1\n",
      "c_left: 0.33333333333333337\n",
      "c_right: -0.3333333333333333\n",
      "Minimaler Fehler: 0.11111111111111112\n",
      "=================================\n",
      "\n",
      "Stump Predictions h_t(x): [-0.3333 -0.3333 -0.3333  0.3333  0.3333  0.3333]\n",
      "Neue Logits F: [-0.8598 -0.8598 -0.8598 -0.5265 -0.5265 -0.5265]\n",
      "\n",
      "=== Iteration 2 ===\n",
      "Aktuelle Logits F: [-0.8598 -0.8598 -0.8598 -0.5265 -0.5265 -0.5265]\n",
      "Aktuelle Wahrsch. p: [0.2974 0.2974 0.2974 0.3713 0.3713 0.3713]\n",
      "Pseudo-Residuen r: [-0.2974 -0.2974 -0.2974  0.6287 -0.3713  0.6287]\n",
      "\n",
      "=== Regression Stump Training ===\n",
      "X: [1. 2. 3. 4. 5. 6.]\n",
      "Zielwerte (Pseudo-Residuen): [-0.2974 -0.2974 -0.2974  0.6287 -0.3713  0.6287]\n",
      "Gewichte: [0.1667 0.1667 0.1667 0.1667 0.1667 0.1667]\n",
      "  thresh=1.0, pol=-1, c_left=0.058, c_right=-0.297, err=0.18137\n",
      "  thresh=2.0, pol=1, c_left=-0.297, c_right=0.058, err=0.18137\n",
      "  thresh=2.0, pol=-1, c_left=0.147, c_right=-0.297, err=0.15502\n",
      "  thresh=3.0, pol=1, c_left=-0.297, c_right=0.147, err=0.15502\n",
      "  thresh=3.0, pol=-1, c_left=0.295, c_right=-0.297, err=0.11111\n",
      "  thresh=4.0, pol=1, c_left=-0.297, c_right=0.295, err=0.11111\n",
      "  thresh=4.0, pol=-1, c_left=0.129, c_right=-0.066, err=0.19053\n",
      "  thresh=5.0, pol=1, c_left=-0.066, c_right=0.129, err=0.19053\n",
      "  thresh=5.0, pol=-1, c_left=0.629, c_right=-0.127, err=0.11964\n",
      "  thresh=6.0, pol=1, c_left=-0.127, c_right=0.629, err=0.11964\n",
      "\n",
      "--> Gewählter Regressions-Stump:\n",
      "Threshold: 3.0\n",
      "Polarity: -1\n",
      "c_left: 0.2953285409742298\n",
      "c_right: -0.2973782397718162\n",
      "Minimaler Fehler: 0.1111111111111111\n",
      "=================================\n",
      "\n",
      "Stump Predictions h_t(x): [-0.2974 -0.2974 -0.2974  0.2953  0.2953  0.2953]\n",
      "Neue Logits F: [-1.0085 -1.0085 -1.0085 -0.3788 -0.3788 -0.3788]\n",
      "\n",
      "=== Iteration 3 ===\n",
      "Aktuelle Logits F: [-1.0085 -1.0085 -1.0085 -0.3788 -0.3788 -0.3788]\n",
      "Aktuelle Wahrsch. p: [0.2673 0.2673 0.2673 0.4064 0.4064 0.4064]\n",
      "Pseudo-Residuen r: [-0.2673 -0.2673 -0.2673  0.5936 -0.4064  0.5936]\n",
      "\n",
      "=== Regression Stump Training ===\n",
      "X: [1. 2. 3. 4. 5. 6.]\n",
      "Zielwerte (Pseudo-Residuen): [-0.2673 -0.2673 -0.2673  0.5936 -0.4064  0.5936]\n",
      "Gewichte: [0.1667 0.1667 0.1667 0.1667 0.1667 0.1667]\n",
      "  thresh=1.0, pol=-1, c_left=0.049, c_right=-0.267, err=0.16677\n",
      "  thresh=2.0, pol=1, c_left=-0.267, c_right=0.049, err=0.16677\n",
      "  thresh=2.0, pol=-1, c_left=0.128, c_right=-0.267, err=0.14590\n",
      "  thresh=3.0, pol=1, c_left=-0.267, c_right=0.128, err=0.14590\n",
      "  thresh=3.0, pol=-1, c_left=0.260, c_right=-0.267, err=0.11111\n",
      "  thresh=4.0, pol=1, c_left=-0.267, c_right=0.260, err=0.11111\n",
      "  thresh=4.0, pol=-1, c_left=0.094, c_right=-0.052, err=0.17597\n",
      "  thresh=5.0, pol=1, c_left=-0.052, c_right=0.094, err=0.17597\n",
      "  thresh=5.0, pol=-1, c_left=0.594, c_right=-0.123, err=0.10938\n",
      "  thresh=6.0, pol=1, c_left=-0.123, c_right=0.594, err=0.10938\n",
      "\n",
      "--> Gewählter Regressions-Stump:\n",
      "Threshold: 5.0\n",
      "Polarity: -1\n",
      "c_left: 0.5935875635409205\n",
      "c_right: -0.12292872939664116\n",
      "Minimaler Fehler: 0.10937739172634667\n",
      "=================================\n",
      "\n",
      "Stump Predictions h_t(x): [-0.1229 -0.1229 -0.1229 -0.1229 -0.1229  0.5936]\n",
      "Neue Logits F: [-1.07   -1.07   -1.07   -0.4403 -0.4403 -0.082 ]\n",
      "\n",
      "========= Training beendet =========\n",
      "\n",
      "Vorhersagewahrscheinlichkeiten: [0.255 0.255 0.255 0.392 0.392 0.48 ]\n",
      "Vorhersagen: [0 0 0 0 0 0]\n",
      "Wahr: [0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[2],[3],[4],[5],[6]], dtype=float)\n",
    "y = np.array([0,0,0,1,0,1], dtype=int)\n",
    "\n",
    "gb = LogisticGradientBoostingDebug(T=3, lr=0.5)\n",
    "gb.fit(X, y)\n",
    "\n",
    "print(\"Vorhersagewahrscheinlichkeiten:\", np.round(gb.predict_proba(X), 3))\n",
    "print(\"Vorhersagen:\", gb.predict(X))\n",
    "print(\"Wahr:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54ac91-0064-4288-b955-223a56b48157",
   "metadata": {},
   "source": [
    "## Warum unser Gradient Boosting mit Decision Stumps auf diesem Datensatz nicht gut funktioniert\n",
    "\n",
    "Wir verwenden hier ein sehr kleines und strukturell schwieriges Beispiel:\n",
    "\n",
    "y = np.array([0,0,0,1,0,1], dtype=int)\n",
    "\n",
    "Die Zielwerte wechseln also zweimal zwischen 0 und 1:  \n",
    "drei Nullen → eine Eins → eine Null → eine Eins.\n",
    "\n",
    "Ein Decision Stump (Baum der Tiefe 1) kann immer nur **eine** Grenze setzen und den Raum in **zwei** Regionen teilen. Damit kann er bestenfalls eine grobe Trennung vornehmen, aber nicht eine Struktur abbilden, in der die Klasse mehrfach hin- und herspringt. Für das Modell sehen die Residuen daher so aus, dass jeder Stump immer nur eine sehr einfache, unzureichende Approximation liefert.\n",
    "\n",
    "Die Folge ist:  \n",
    "Auch wenn das Gradient Boosting formal korrekt arbeitet, liefern die Stumps in jeder Iteration **zu wenig Information**, um die Logits sinnvoll in Richtung der richtigen Klassen zu bewegen. Das Modell macht zwar Updates, aber diese reichen nicht aus, um die beiden „eingestreuten“ positiven Beispiele korrekt zu erkennen.\n",
    "\n",
    "Deshalb führt der Algorithmus zwar mehrere Iterationen aus, aber die Vorhersagen verbessern sich kaum.\n",
    "\n",
    "Die Lösung besteht darin, den Base Learner etwas stärker zu machen:  \n",
    "Statt eines Stumps verwenden wir einen Baum mit größerer Tiefe (z. B. depth = 2). Ein solcher Baum kann bereits zwei Splits setzen und damit **drei Regionen** modellieren – genug, um diese einfache, aber nicht stumpf trennbare Struktur zu approximieren.\n",
    "\n",
    "Darum führen wir den Hyperparameter `max_depth` ein:  \n",
    "Damit können wir den Base Learner stärker machen und dem Gradient Boosting überhaupt erst ermöglichen, die Struktur des Datensatzes zu „sehen“. Für die Studierenden ist das oft das Aha-Erlebnis: Boosting funktioniert nicht automatisch – die Wahl des Base Learners entscheidet, ob das Modell überhaupt lernfähig ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1ec77dc-4ed9-4ffc-a1ee-00228770d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTreeDebug:\n",
    "    \"\"\"\n",
    "    Ein generischer Regressionsbaum für Gradient Boosting:\n",
    "    - beliebige max_depth\n",
    "    - weighted variance split\n",
    "    - jeder Leaf = weighted mean\n",
    "    - komplette Debug-Ausgabe\n",
    "    \"\"\"\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, is_leaf=False, value=None,\n",
    "                     thresh=None, pol=None, left=None, right=None):\n",
    "            self.is_leaf = is_leaf\n",
    "            self.value = value\n",
    "            self.thresh = thresh\n",
    "            self.pol = pol\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y, w=None):\n",
    "        X = X.flatten()\n",
    "        y = np.array(y, float)\n",
    "        if w is None:\n",
    "            w = np.ones_like(y) / len(y)\n",
    "\n",
    "        print(f\"\\n====== Train Regression Tree (max_depth={self.max_depth}) ======\")\n",
    "        self.root = self._build_tree(X, y, w, depth=1)\n",
    "        print(\"==============================================================\\n\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    #   Recursive Tree Builder\n",
    "    # -------------------------------------------------------------\n",
    "    def _build_tree(self, X, y, w, depth):\n",
    "        print(f\"\\n--> Baue Node auf depth={depth}, n={len(y)}\")\n",
    "\n",
    "        # Leaf stopping conditions\n",
    "        if depth > self.max_depth or len(y) <= 1:\n",
    "            leaf_value = np.sum(w * y) / np.sum(w)\n",
    "            print(f\"   LEAF: value={leaf_value:.4f}\")\n",
    "            return self.Node(is_leaf=True, value=leaf_value)\n",
    "\n",
    "        # best split search\n",
    "        best_err = np.inf\n",
    "        best_thresh = None\n",
    "        best_pol = None\n",
    "        best_left_mask = None\n",
    "        best_right_mask = None\n",
    "\n",
    "        print(\"   Suche besten Split...\")\n",
    "\n",
    "        for thresh in np.unique(X):\n",
    "            for pol in [1, -1]:\n",
    "                left_mask = (X * pol) < (thresh * pol)\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # weighted variance error\n",
    "                err = (\n",
    "                    np.sum(w[left_mask]  * (y[left_mask]  - np.mean(y[left_mask]))**2) +\n",
    "                    np.sum(w[right_mask] * (y[right_mask] - np.mean(y[right_mask]))**2)\n",
    "                )\n",
    "\n",
    "                print(f\"     Test: thresh={thresh}, pol={pol}, err={err:.5f}\")\n",
    "\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    best_thresh = thresh\n",
    "                    best_pol = pol\n",
    "                    best_left_mask = left_mask\n",
    "                    best_right_mask = right_mask\n",
    "\n",
    "        # if no split works => leaf\n",
    "        if best_thresh is None:\n",
    "            leaf_value = np.sum(w * y) / np.sum(w)\n",
    "            print(f\"   (keine sinnvollen Splits) LEAF={leaf_value:.4f}\")\n",
    "            return self.Node(is_leaf=True, value=leaf_value)\n",
    "\n",
    "        print(f\"   -> Bester Split: thresh={best_thresh}, pol={best_pol}, err={best_err:.5f}\")\n",
    "\n",
    "        # recursively build children\n",
    "        left_node = self._build_tree(X[best_left_mask],\n",
    "                                     y[best_left_mask],\n",
    "                                     w[best_left_mask],\n",
    "                                     depth + 1)\n",
    "\n",
    "        right_node = self._build_tree(X[best_right_mask],\n",
    "                                      y[best_right_mask],\n",
    "                                      w[best_right_mask],\n",
    "                                      depth + 1)\n",
    "\n",
    "        return self.Node(is_leaf=False,\n",
    "                         thresh=best_thresh,\n",
    "                         pol=best_pol,\n",
    "                         left=left_node,\n",
    "                         right=right_node)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    #   Prediction\n",
    "    # -------------------------------------------------------------\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.value\n",
    "\n",
    "        if (x * node.pol) < (node.thresh * node.pol):\n",
    "            return self._predict_one(x, node.left)\n",
    "        else:\n",
    "            return self._predict_one(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.flatten()\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13e14192-1b9b-41c3-8a9f-0493009d3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticGradientBoostingDebug:\n",
    "\n",
    "    def __init__(self, T=5, lr=0.5, max_depth=1):\n",
    "        \"\"\"\n",
    "        T         = Anzahl der Boosting-Stufen\n",
    "        lr        = Lernrate\n",
    "        max_depth = Maximal-Tiefe des Base Learners (Regressionsbaum)\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.lr = lr\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "        # Start-F0 = log-odds der Grundwahrscheinlichkeit\n",
    "        p0 = np.mean(y)\n",
    "        self.F0 = np.log(p0 / (1 - p0))\n",
    "        F = np.ones(len(y)) * self.F0\n",
    "\n",
    "        print(\"\\n========== Logistic Gradient Boosting Training ==========\")\n",
    "        print(f\"Startwert F0 (log-odds) = {self.F0:.4f}\")\n",
    "        print(f\"Base Learner max_depth = {self.max_depth}\\n\")\n",
    "\n",
    "        for t in range(1, self.T+1):\n",
    "            print(f\"\\n=== Iteration {t} ===\")\n",
    "\n",
    "            # probabilistische predictions\n",
    "            p = sigmoid(F)\n",
    "            r = y - p\n",
    "\n",
    "            print(\"Aktuelle Logits F:\", np.round(F, 4))\n",
    "            print(\"Aktuelle Wahrsch. p:\", np.round(p, 4))\n",
    "            print(\"Pseudo-Residuen r:\", np.round(r, 4))\n",
    "\n",
    "            # --- HIER wird der Base-Learner mit max_depth erzeugt ---\n",
    "            stump = RegressionTreeDebug(max_depth=self.max_depth)\n",
    "            stump.fit(X, r)\n",
    "\n",
    "            pred = stump.predict(X)\n",
    "            print(\"h_t(x):\", np.round(pred, 4))\n",
    "\n",
    "            F = F + self.lr * pred\n",
    "            print(\"Neue Logits F:\", np.round(F, 4))\n",
    "\n",
    "            self.models.append(stump)\n",
    "\n",
    "        print(\"\\n========= Training beendet =========\\n\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        F = np.ones(len(X)) * self.F0\n",
    "        for stump in self.models:\n",
    "            F += self.lr * stump.predict(X)\n",
    "        return sigmoid(F)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40d62c66-1bf7-4201-abfa-08d791dc70d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Logistic Gradient Boosting Training ==========\n",
      "Startwert F0 (log-odds) = -0.6931\n",
      "Base Learner max_depth = 2\n",
      "\n",
      "\n",
      "=== Iteration 1 ===\n",
      "Aktuelle Logits F: [-0.6931 -0.6931 -0.6931 -0.6931 -0.6931 -0.6931]\n",
      "Aktuelle Wahrsch. p: [0.3333 0.3333 0.3333 0.3333 0.3333 0.3333]\n",
      "Pseudo-Residuen r: [-0.3333 -0.3333 -0.3333  0.6667 -0.3333  0.6667]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.20000\n",
      "     Test: thresh=2, pol=1, err=0.20000\n",
      "     Test: thresh=2, pol=-1, err=0.16667\n",
      "     Test: thresh=3, pol=1, err=0.16667\n",
      "     Test: thresh=3, pol=-1, err=0.11111\n",
      "     Test: thresh=4, pol=1, err=0.11111\n",
      "     Test: thresh=4, pol=-1, err=0.20833\n",
      "     Test: thresh=5, pol=1, err=0.20833\n",
      "     Test: thresh=5, pol=-1, err=0.13333\n",
      "     Test: thresh=6, pol=1, err=0.13333\n",
      "   -> Bester Split: thresh=3, pol=-1, err=0.11111\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=4, pol=-1, err=0.08333\n",
      "     Test: thresh=5, pol=1, err=0.08333\n",
      "     Test: thresh=5, pol=-1, err=0.08333\n",
      "     Test: thresh=6, pol=1, err=0.08333\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.08333\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=0.1667\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=0.6667\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.00000\n",
      "     Test: thresh=2, pol=1, err=0.00000\n",
      "     Test: thresh=2, pol=-1, err=0.00000\n",
      "     Test: thresh=3, pol=1, err=0.00000\n",
      "   -> Bester Split: thresh=1, pol=-1, err=0.00000\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=-0.3333\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.3333\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.3333 -0.3333 -0.3333  0.6667  0.1667  0.1667]\n",
      "Neue Logits F: [-0.8598 -0.8598 -0.8598 -0.3598 -0.6098 -0.6098]\n",
      "\n",
      "=== Iteration 2 ===\n",
      "Aktuelle Logits F: [-0.8598 -0.8598 -0.8598 -0.3598 -0.6098 -0.6098]\n",
      "Aktuelle Wahrsch. p: [0.2974 0.2974 0.2974 0.411  0.3521 0.3521]\n",
      "Pseudo-Residuen r: [-0.2974 -0.2974 -0.2974  0.589  -0.3521  0.6479]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.17512\n",
      "     Test: thresh=2, pol=1, err=0.17512\n",
      "     Test: thresh=2, pol=-1, err=0.14881\n",
      "     Test: thresh=3, pol=1, err=0.14881\n",
      "     Test: thresh=3, pol=-1, err=0.10495\n",
      "     Test: thresh=4, pol=1, err=0.10495\n",
      "     Test: thresh=4, pol=-1, err=0.18154\n",
      "     Test: thresh=5, pol=1, err=0.18154\n",
      "     Test: thresh=5, pol=-1, err=0.10839\n",
      "     Test: thresh=6, pol=1, err=0.10839\n",
      "   -> Bester Split: thresh=3, pol=-1, err=0.10495\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=4, pol=-1, err=0.08333\n",
      "     Test: thresh=5, pol=1, err=0.08333\n",
      "     Test: thresh=5, pol=-1, err=0.07381\n",
      "     Test: thresh=6, pol=1, err=0.07381\n",
      "   -> Bester Split: thresh=5, pol=-1, err=0.07381\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=0.6479\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=0.1184\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.00000\n",
      "     Test: thresh=2, pol=1, err=0.00000\n",
      "     Test: thresh=2, pol=-1, err=0.00000\n",
      "     Test: thresh=3, pol=1, err=0.00000\n",
      "   -> Bester Split: thresh=1, pol=-1, err=0.00000\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=-0.2974\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.2974\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.2974 -0.2974 -0.2974  0.1184  0.1184  0.6479]\n",
      "Neue Logits F: [-1.0085 -1.0085 -1.0085 -0.3006 -0.5506 -0.2859]\n",
      "\n",
      "=== Iteration 3 ===\n",
      "Aktuelle Logits F: [-1.0085 -1.0085 -1.0085 -0.3006 -0.5506 -0.2859]\n",
      "Aktuelle Wahrsch. p: [0.2673 0.2673 0.2673 0.4254 0.3657 0.429 ]\n",
      "Pseudo-Residuen r: [-0.2673 -0.2673 -0.2673  0.5746 -0.3657  0.571 ]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.15346\n",
      "     Test: thresh=2, pol=1, err=0.15346\n",
      "     Test: thresh=2, pol=-1, err=0.13261\n",
      "     Test: thresh=3, pol=1, err=0.13261\n",
      "     Test: thresh=3, pol=-1, err=0.09787\n",
      "     Test: thresh=4, pol=1, err=0.09787\n",
      "     Test: thresh=4, pol=-1, err=0.16171\n",
      "     Test: thresh=5, pol=1, err=0.16171\n",
      "     Test: thresh=5, pol=-1, err=0.10132\n",
      "     Test: thresh=6, pol=1, err=0.10132\n",
      "   -> Bester Split: thresh=3, pol=-1, err=0.09787\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=4, pol=-1, err=0.07312\n",
      "     Test: thresh=5, pol=1, err=0.07312\n",
      "     Test: thresh=5, pol=-1, err=0.07368\n",
      "     Test: thresh=6, pol=1, err=0.07368\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.07312\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=0.1026\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=0.5746\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.00000\n",
      "     Test: thresh=2, pol=1, err=0.00000\n",
      "     Test: thresh=2, pol=-1, err=0.00000\n",
      "     Test: thresh=3, pol=1, err=0.00000\n",
      "   -> Bester Split: thresh=1, pol=-1, err=0.00000\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=-0.2673\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.2673\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.2673 -0.2673 -0.2673  0.5746  0.1026  0.1026]\n",
      "Neue Logits F: [-1.1421 -1.1421 -1.1421 -0.0133 -0.4993 -0.2346]\n",
      "\n",
      "=== Iteration 4 ===\n",
      "Aktuelle Logits F: [-1.1421 -1.1421 -1.1421 -0.0133 -0.4993 -0.2346]\n",
      "Aktuelle Wahrsch. p: [0.2419 0.2419 0.2419 0.4967 0.3777 0.4416]\n",
      "Pseudo-Residuen r: [-0.2419 -0.2419 -0.2419  0.5033 -0.3777  0.5584]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.13614\n",
      "     Test: thresh=2, pol=1, err=0.13614\n",
      "     Test: thresh=2, pol=-1, err=0.11958\n",
      "     Test: thresh=3, pol=1, err=0.11958\n",
      "     Test: thresh=3, pol=-1, err=0.09197\n",
      "     Test: thresh=4, pol=1, err=0.09197\n",
      "     Test: thresh=4, pol=-1, err=0.14245\n",
      "     Test: thresh=5, pol=1, err=0.14245\n",
      "     Test: thresh=5, pol=-1, err=0.08326\n",
      "     Test: thresh=6, pol=1, err=0.08326\n",
      "   -> Bester Split: thresh=5, pol=-1, err=0.08326\n",
      "\n",
      "--> Baue Node auf depth=2, n=1\n",
      "   LEAF: value=0.5584\n",
      "\n",
      "--> Baue Node auf depth=2, n=5\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.08016\n",
      "     Test: thresh=2, pol=1, err=0.08016\n",
      "     Test: thresh=2, pol=-1, err=0.07500\n",
      "     Test: thresh=3, pol=1, err=0.07500\n",
      "     Test: thresh=3, pol=-1, err=0.06469\n",
      "     Test: thresh=4, pol=1, err=0.06469\n",
      "     Test: thresh=4, pol=-1, err=0.06943\n",
      "     Test: thresh=5, pol=1, err=0.06943\n",
      "   -> Bester Split: thresh=3, pol=-1, err=0.06469\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=0.0628\n",
      "\n",
      "--> Baue Node auf depth=3, n=3\n",
      "   LEAF: value=-0.2419\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.2419 -0.2419 -0.2419  0.0628  0.0628  0.5584]\n",
      "Neue Logits F: [-1.2631 -1.2631 -1.2631  0.0181 -0.4679  0.0446]\n",
      "\n",
      "=== Iteration 5 ===\n",
      "Aktuelle Logits F: [-1.2631 -1.2631 -1.2631  0.0181 -0.4679  0.0446]\n",
      "Aktuelle Wahrsch. p: [0.2204 0.2204 0.2204 0.5045 0.3851 0.5112]\n",
      "Pseudo-Residuen r: [-0.2204 -0.2204 -0.2204  0.4955 -0.3851  0.4888]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.12083\n",
      "     Test: thresh=2, pol=1, err=0.12083\n",
      "     Test: thresh=2, pol=-1, err=0.10758\n",
      "     Test: thresh=3, pol=1, err=0.10758\n",
      "     Test: thresh=3, pol=-1, err=0.08552\n",
      "     Test: thresh=4, pol=1, err=0.08552\n",
      "     Test: thresh=4, pol=-1, err=0.12772\n",
      "     Test: thresh=5, pol=1, err=0.12772\n",
      "     Test: thresh=5, pol=-1, err=0.07981\n",
      "     Test: thresh=6, pol=1, err=0.07981\n",
      "   -> Bester Split: thresh=5, pol=-1, err=0.07981\n",
      "\n",
      "--> Baue Node auf depth=2, n=1\n",
      "   LEAF: value=0.4888\n",
      "\n",
      "--> Baue Node auf depth=2, n=5\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.07728\n",
      "     Test: thresh=2, pol=1, err=0.07728\n",
      "     Test: thresh=2, pol=-1, err=0.07306\n",
      "     Test: thresh=3, pol=1, err=0.07306\n",
      "     Test: thresh=3, pol=-1, err=0.06462\n",
      "     Test: thresh=4, pol=1, err=0.06462\n",
      "     Test: thresh=4, pol=-1, err=0.06407\n",
      "     Test: thresh=5, pol=1, err=0.06407\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.06407\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.3851\n",
      "\n",
      "--> Baue Node auf depth=3, n=4\n",
      "   LEAF: value=-0.0415\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.0415 -0.0415 -0.0415 -0.0415 -0.3851  0.4888]\n",
      "Neue Logits F: [-1.2838 -1.2838 -1.2838 -0.0026 -0.6604  0.2891]\n",
      "\n",
      "=== Iteration 6 ===\n",
      "Aktuelle Logits F: [-1.2838 -1.2838 -1.2838 -0.0026 -0.6604  0.2891]\n",
      "Aktuelle Wahrsch. p: [0.2169 0.2169 0.2169 0.4993 0.3406 0.5718]\n",
      "Pseudo-Residuen r: [-0.2169 -0.2169 -0.2169  0.5007 -0.3406  0.4282]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.10657\n",
      "     Test: thresh=2, pol=1, err=0.10657\n",
      "     Test: thresh=2, pol=-1, err=0.09377\n",
      "     Test: thresh=3, pol=1, err=0.09377\n",
      "     Test: thresh=3, pol=-1, err=0.07246\n",
      "     Test: thresh=4, pol=1, err=0.07246\n",
      "     Test: thresh=4, pol=-1, err=0.11362\n",
      "     Test: thresh=5, pol=1, err=0.11362\n",
      "     Test: thresh=5, pol=-1, err=0.07661\n",
      "     Test: thresh=6, pol=1, err=0.07661\n",
      "   -> Bester Split: thresh=3, pol=-1, err=0.07246\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=4, pol=-1, err=0.04926\n",
      "     Test: thresh=5, pol=1, err=0.04926\n",
      "     Test: thresh=5, pol=-1, err=0.05898\n",
      "     Test: thresh=6, pol=1, err=0.05898\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.04926\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=0.0438\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=0.5007\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.00000\n",
      "     Test: thresh=2, pol=1, err=0.00000\n",
      "     Test: thresh=2, pol=-1, err=0.00000\n",
      "     Test: thresh=3, pol=1, err=0.00000\n",
      "   -> Bester Split: thresh=1, pol=-1, err=0.00000\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=-0.2169\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.2169\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.2169 -0.2169 -0.2169  0.5007  0.0438  0.0438]\n",
      "Neue Logits F: [-1.3923 -1.3923 -1.3923  0.2477 -0.6385  0.311 ]\n",
      "\n",
      "=== Iteration 7 ===\n",
      "Aktuelle Logits F: [-1.3923 -1.3923 -1.3923  0.2477 -0.6385  0.311 ]\n",
      "Aktuelle Wahrsch. p: [0.199  0.199  0.199  0.5616 0.3456 0.5771]\n",
      "Pseudo-Residuen r: [-0.199  -0.199  -0.199   0.4384 -0.3456  0.4229]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.09448\n",
      "     Test: thresh=2, pol=1, err=0.09448\n",
      "     Test: thresh=2, pol=-1, err=0.08416\n",
      "     Test: thresh=3, pol=1, err=0.08416\n",
      "     Test: thresh=3, pol=-1, err=0.06697\n",
      "     Test: thresh=4, pol=1, err=0.06697\n",
      "     Test: thresh=4, pol=-1, err=0.10000\n",
      "     Test: thresh=5, pol=1, err=0.10000\n",
      "     Test: thresh=5, pol=-1, err=0.06327\n",
      "     Test: thresh=6, pol=1, err=0.06327\n",
      "   -> Bester Split: thresh=5, pol=-1, err=0.06327\n",
      "\n",
      "--> Baue Node auf depth=2, n=1\n",
      "   LEAF: value=0.4229\n",
      "\n",
      "--> Baue Node auf depth=2, n=5\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.06126\n",
      "     Test: thresh=2, pol=1, err=0.06126\n",
      "     Test: thresh=2, pol=-1, err=0.05791\n",
      "     Test: thresh=3, pol=1, err=0.05791\n",
      "     Test: thresh=3, pol=-1, err=0.05122\n",
      "     Test: thresh=4, pol=1, err=0.05122\n",
      "     Test: thresh=4, pol=-1, err=0.05079\n",
      "     Test: thresh=5, pol=1, err=0.05079\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.05079\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.3456\n",
      "\n",
      "--> Baue Node auf depth=3, n=4\n",
      "   LEAF: value=-0.0397\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.0397 -0.0397 -0.0397 -0.0397 -0.3456  0.4229]\n",
      "Neue Logits F: [-1.4121 -1.4121 -1.4121  0.2279 -0.8113  0.5224]\n",
      "\n",
      "=== Iteration 8 ===\n",
      "Aktuelle Logits F: [-1.4121 -1.4121 -1.4121  0.2279 -0.8113  0.5224]\n",
      "Aktuelle Wahrsch. p: [0.1959 0.1959 0.1959 0.5567 0.3076 0.6277]\n",
      "Pseudo-Residuen r: [-0.1959 -0.1959 -0.1959  0.4433 -0.3076  0.3723]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.08396\n",
      "     Test: thresh=2, pol=1, err=0.08396\n",
      "     Test: thresh=2, pol=-1, err=0.07396\n",
      "     Test: thresh=3, pol=1, err=0.07396\n",
      "     Test: thresh=3, pol=-1, err=0.05729\n",
      "     Test: thresh=4, pol=1, err=0.05729\n",
      "     Test: thresh=4, pol=-1, err=0.08959\n",
      "     Test: thresh=5, pol=1, err=0.08959\n",
      "     Test: thresh=5, pol=-1, err=0.06090\n",
      "     Test: thresh=6, pol=1, err=0.06090\n",
      "   -> Bester Split: thresh=3, pol=-1, err=0.05729\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=4, pol=-1, err=0.03852\n",
      "     Test: thresh=5, pol=1, err=0.03852\n",
      "     Test: thresh=5, pol=-1, err=0.04699\n",
      "     Test: thresh=6, pol=1, err=0.04699\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.03852\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=0.0323\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=0.4433\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.00000\n",
      "     Test: thresh=2, pol=1, err=0.00000\n",
      "     Test: thresh=2, pol=-1, err=0.00000\n",
      "     Test: thresh=3, pol=1, err=0.00000\n",
      "   -> Bester Split: thresh=1, pol=-1, err=0.00000\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=-0.1959\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.1959\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.1959 -0.1959 -0.1959  0.4433  0.0323  0.0323]\n",
      "Neue Logits F: [-1.5101 -1.5101 -1.5101  0.4495 -0.7952  0.5386]\n",
      "\n",
      "=== Iteration 9 ===\n",
      "Aktuelle Logits F: [-1.5101 -1.5101 -1.5101  0.4495 -0.7952  0.5386]\n",
      "Aktuelle Wahrsch. p: [0.1809 0.1809 0.1809 0.6105 0.3111 0.6315]\n",
      "Pseudo-Residuen r: [-0.1809 -0.1809 -0.1809  0.3895 -0.3111  0.3685]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.07471\n",
      "     Test: thresh=2, pol=1, err=0.07471\n",
      "     Test: thresh=2, pol=-1, err=0.06655\n",
      "     Test: thresh=3, pol=1, err=0.06655\n",
      "     Test: thresh=3, pol=-1, err=0.05295\n",
      "     Test: thresh=4, pol=1, err=0.05295\n",
      "     Test: thresh=4, pol=-1, err=0.07916\n",
      "     Test: thresh=5, pol=1, err=0.07916\n",
      "     Test: thresh=5, pol=-1, err=0.05059\n",
      "     Test: thresh=6, pol=1, err=0.05059\n",
      "   -> Bester Split: thresh=5, pol=-1, err=0.05059\n",
      "\n",
      "--> Baue Node auf depth=2, n=1\n",
      "   LEAF: value=0.3685\n",
      "\n",
      "--> Baue Node auf depth=2, n=5\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.04897\n",
      "     Test: thresh=2, pol=1, err=0.04897\n",
      "     Test: thresh=2, pol=-1, err=0.04628\n",
      "     Test: thresh=3, pol=1, err=0.04628\n",
      "     Test: thresh=3, pol=-1, err=0.04090\n",
      "     Test: thresh=4, pol=1, err=0.04090\n",
      "     Test: thresh=4, pol=-1, err=0.04067\n",
      "     Test: thresh=5, pol=1, err=0.04067\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.04067\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.3111\n",
      "\n",
      "--> Baue Node auf depth=3, n=4\n",
      "   LEAF: value=-0.0383\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.0383 -0.0383 -0.0383 -0.0383 -0.3111  0.3685]\n",
      "Neue Logits F: [-1.5292 -1.5292 -1.5292  0.4303 -0.9507  0.7228]\n",
      "\n",
      "=== Iteration 10 ===\n",
      "Aktuelle Logits F: [-1.5292 -1.5292 -1.5292  0.4303 -0.9507  0.7228]\n",
      "Aktuelle Wahrsch. p: [0.1781 0.1781 0.1781 0.606  0.2787 0.6732]\n",
      "Pseudo-Residuen r: [-0.1781 -0.1781 -0.1781  0.394  -0.2787  0.3268]\n",
      "\n",
      "====== Train Regression Tree (max_depth=2) ======\n",
      "\n",
      "--> Baue Node auf depth=1, n=6\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.06695\n",
      "     Test: thresh=2, pol=1, err=0.06695\n",
      "     Test: thresh=2, pol=-1, err=0.05901\n",
      "     Test: thresh=3, pol=1, err=0.05901\n",
      "     Test: thresh=3, pol=-1, err=0.04577\n",
      "     Test: thresh=4, pol=1, err=0.04577\n",
      "     Test: thresh=4, pol=-1, err=0.07147\n",
      "     Test: thresh=5, pol=1, err=0.07147\n",
      "     Test: thresh=5, pol=-1, err=0.04884\n",
      "     Test: thresh=6, pol=1, err=0.04884\n",
      "   -> Bester Split: thresh=3, pol=-1, err=0.04577\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=4, pol=-1, err=0.03055\n",
      "     Test: thresh=5, pol=1, err=0.03055\n",
      "     Test: thresh=5, pol=-1, err=0.03772\n",
      "     Test: thresh=6, pol=1, err=0.03772\n",
      "   -> Bester Split: thresh=4, pol=-1, err=0.03055\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=0.0240\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=0.3940\n",
      "\n",
      "--> Baue Node auf depth=2, n=3\n",
      "   Suche besten Split...\n",
      "     Test: thresh=1, pol=-1, err=0.00000\n",
      "     Test: thresh=2, pol=1, err=0.00000\n",
      "     Test: thresh=2, pol=-1, err=0.00000\n",
      "     Test: thresh=3, pol=1, err=0.00000\n",
      "   -> Bester Split: thresh=1, pol=-1, err=0.00000\n",
      "\n",
      "--> Baue Node auf depth=3, n=2\n",
      "   LEAF: value=-0.1781\n",
      "\n",
      "--> Baue Node auf depth=3, n=1\n",
      "   LEAF: value=-0.1781\n",
      "==============================================================\n",
      "\n",
      "h_t(x): [-0.1781 -0.1781 -0.1781  0.394   0.024   0.024 ]\n",
      "Neue Logits F: [-1.6183 -1.6183 -1.6183  0.6274 -0.9387  0.7348]\n",
      "\n",
      "========= Training beendet =========\n",
      "\n",
      "Pred: [0 0 0 1 0 1]\n",
      "True: [0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[2],[3],[4],[5],[6]])\n",
    "y = np.array([0,0,0,1,0,1])\n",
    "\n",
    "gb = LogisticGradientBoostingDebug(\n",
    "    T=10,\n",
    "    lr=0.5,\n",
    "    max_depth=2\n",
    ")\n",
    "\n",
    "gb.fit(X, y)\n",
    "\n",
    "print(\"Pred:\", gb.predict(X))\n",
    "print(\"True:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff42e9-2402-4fe1-961b-6be3628dc498",
   "metadata": {},
   "source": [
    "# Gradient Boosting vs. XGBoost: Eine vergleichende Erklärung\n",
    "\n",
    "Gradient Boosting (GBM) und XGBoost gehören zur gleichen Familie: Beide sind **additive Modelle**, die schwache Lerner (normalerweise Entscheidungsbäume) schrittweise aufbauen, indem sie Fehler der vorherigen Iterationen korrigieren.  \n",
    "XGBoost ist jedoch **nicht einfach „Gradient Boosting, aber schneller“** – es erweitert das klassische Gradient Boosting um wichtige Verbesserungen in Optimierung, Regularisierung und Robustheit.\n",
    "\n",
    "## 1. Gradient Boosting: Optimierung über negative Gradienten\n",
    "\n",
    "Gradient Boosting ist das allgemeine Framework.  \n",
    "In jeder Iteration wird ein schwacher Lerner auf den **Pseudo-Residuen** trainiert:\n",
    "\n",
    "$$g_i = -\\frac{\\partial \\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)}.$$\n",
    "\n",
    "Der Baum versucht optimal abzubilden, wie der Gesamtfehler (Loss) am stärksten reduziert werden kann.\n",
    "\n",
    "### Zentrale Eigenschaften von klassischem GBM\n",
    "* Bäume approximieren **nur** den **1. Ableitungsfehler** (negative Gradienten).\n",
    "* Keine explizite Regularisierung in der Loss-Form.\n",
    "* Leaf-Werte werden als einfacher gewichteter Mittelwert der Residuen bestimmt.\n",
    "* Splits basieren auf **Fehlerreduktion** (z. B. Varianzreduktion oder Entropieersparnis).\n",
    "* GBM ist rein funktional auf das Minimieren der Loss ausgerichtet – ohne strukturelle Nebenbedingungen.\n",
    "\n",
    "Gradient Boosting ist dadurch flexibel, aber empfindlich gegenüber:\n",
    "- Rauschen,  \n",
    "- zu tiefen Bäumen,  \n",
    "- komplexen Residuen,  \n",
    "- Overfitting auf kleinen Datensätzen.\n",
    "\n",
    "\n",
    "## 2. XGBoost: Gradient Boosting mit 2. Ableitung, Regularisierung und Optimierung\n",
    "\n",
    "XGBoost erweitert das klassische GBM in drei wesentlichen Dimensionen:\n",
    "\n",
    "### (a) Optimierung mit 2. Ordnung (Newton-Schritt)\n",
    "XGBoost nutzt nicht nur den ersten Gradienten \\(g_i\\), sondern auch die **zweite Ableitung** (Hessian):\n",
    "\n",
    "$$h_i = \\frac{\\partial^2 \\mathcal{L}}{\\partial F(x_i)^2}.$$\n",
    "\n",
    "Damit stellt XGBoost eine **zweite Ordnung Taylor-Approximation** des Loss bereit.  \n",
    "Die Leaf-Werte werden nach einer exakten Formel berechnet:\n",
    "\n",
    "$$\n",
    "w = -\\frac{\\sum_i g_i}{\\sum_i h_i + \\lambda}\n",
    "$$\n",
    "\n",
    "Das bedeutet:\n",
    "- tiefere, „präzisere“ Updates\n",
    "- bessere Konvergenz\n",
    "- stabilere Trainingsschritte\n",
    "\n",
    "Das ist wesentlich genauer als die „Mittelwert-Update“-Logik von einfachem Gradient Boosting.\n",
    "\n",
    "\n",
    "### (b) Struktur- und Gewichtregularisierung\n",
    "XGBoost reguliert das Baumwachstum explizit.  \n",
    "Die Trainingsobjective enthält zusätzlich:\n",
    "\n",
    "- **L2-Regularisierung** der Leaf-Werte \\( \\lambda w^2 \\)\n",
    "- **L1-Regularisierung** (optional)\n",
    "- **Penalty auf die Anzahl der Leaves** (γ)\n",
    "\n",
    "Damit kontrolliert XGBoost Überanpassung bereits **während** des Baumwachstums.\n",
    "\n",
    "Gradient Boosting geregelt: nur über Lernrate & Baumtiefe  \n",
    "XGBoost geregelt: zusätzlich Leaf-Regularisierung, Strukturkosten, Child-Weight-Bedingungen\n",
    "\n",
    "\n",
    "### (c) Strikte Split-Kriterien\n",
    "Ein Split wird nur akzeptiert, wenn der Gain:\n",
    "\n",
    "$$\n",
    "\\text{Gain} > 0\n",
    "$$\n",
    "\n",
    "ist.  \n",
    "Der Gain beruht auf der 2. Ordnung und enthält Regularisierung:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2}\n",
    "\\left[\n",
    "   \\frac{G_L^2}{H_L+\\lambda}\n",
    " + \\frac{G_R^2}{H_R+\\lambda}\n",
    " - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
    "\\right] - \\gamma\n",
    "$$\n",
    "\n",
    "Damit ignoriert XGBoost alle Splits, die nicht wirklich nützlich sind.  \n",
    "Gradient Boosting splitet dagegen rein auf Residuenoptimierung (ohne solche Hürden).\n",
    "\n",
    "\n",
    "### (d) Leaf-Gewichte werden über Newton-Schritte aktualisiert\n",
    "GBM:  \n",
    "Leaf-Wert = Mittelwert der Residuen (nur 1. Ordnung)\n",
    "\n",
    "XGBoost:  \n",
    "Leaf-Wert = geschlossener Newton-Schritt (1. + 2. Ordnung)\n",
    "\n",
    "Das ergibt oft deutlich stabilere Updates.\n",
    "\n",
    "\n",
    "## 3. Direkter Vergleich\n",
    "\n",
    "| Merkmal | **Gradient Boosting (GBM)** | **XGBoost** |\n",
    "|--------|------------------------------|--------------|\n",
    "| Optimierungsverfahren | 1. Ableitung (Gradient) | 1. + 2. Ableitung (Newton-Schritt) |\n",
    "| Leaf-Berechnung | Mittelwert der Residuen | Geschlossene Formel mit Hessian und λ |\n",
    "| Overfitting-Kontrolle | nur über Lernrate/Depth | L1/L2-Regularisierung, γ-Penalty, Child-Weight |\n",
    "| Split-Kriterien | reine Fehlerreduktion | Gain-Formel mit Regularisierung |\n",
    "| Umgang mit kleinen Leafs | erlaubt | verhindert, wenn Hessians zu klein |\n",
    "| Stabilität | anfälliger | robuster durch 2. Ordnung |\n",
    "| Geschwindigkeit | oft langsamer | optimierte Implementierung (nicht nur Theorie) |\n",
    "\n",
    "\n",
    "## 4. Kernaussage\n",
    "\n",
    "Gradient Boosting ist ein allgemeiner Ansatz: „Baue das Modell, indem du Residuen annäherst.“\n",
    "\n",
    "XGBoost ist die verfeinerte, regulierte, stabilisierte und Newton-optimierte Version davon:  \n",
    "„Baue das Modell effizient, streng reguliert, mit 1. + 2. Ableitung, um Überfitting und instabile Updates zu vermeiden.“\n",
    "\n",
    "Man kann es so zusammenfassen:\n",
    "\n",
    "**GBM = Gradient-Schritt**  \n",
    "**XGBoost = Newton-Schritt + Regularisierung + Optimierter Baumaufbau**\n",
    "\n",
    "Damit ist XGBoost eine direkte Weiterentwicklung des klassischen Gradient Boosting – mit klaren Vorteilen in Stabilität, Genauigkeit und Robustheit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b03311-b446-4d7d-af4c-b38b17c3e4f1",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "092ee8f8-6ee0-4f64-809b-a7a125069e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBTree:\n",
    "    class Node:\n",
    "        def __init__(self, is_leaf=False, value=None,\n",
    "                     feature=None, thresh=None, left=None, right=None):\n",
    "            self.is_leaf = is_leaf\n",
    "            self.value = value\n",
    "            self.feature = feature\n",
    "            self.thresh = thresh\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "    def __init__(self, max_depth=3, lambda_=1.0, min_child_weight=1.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.root = None\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    #           FIT (Newton, Gain-basiert)\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, X, g, h):\n",
    "        X = np.array(X)\n",
    "        self.root = self._build_tree(X, g, h, depth=1)\n",
    "        return self\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    #         Bilde einen Baum rekursiv\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_tree(self, X, g, h, depth):\n",
    "\n",
    "        print(f\"\\n--- Build node: depth={depth}, n={len(X)} ---\")\n",
    "\n",
    "        # Leaf criteria\n",
    "        if depth > self.max_depth or len(X) <= 1 or np.sum(h) < self.min_child_weight:\n",
    "            leaf_value = -np.sum(g) / (np.sum(h) + self.lambda_)\n",
    "            print(f\"  Leaf: value={leaf_value:.4f}\")\n",
    "            return self.Node(is_leaf=True, value=leaf_value)\n",
    "\n",
    "        best_gain = 0\n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "        best_masks = None\n",
    "\n",
    "        # Try all features + thresholds\n",
    "        for feature in range(X.shape[1]):\n",
    "            xs = X[:, feature]\n",
    "\n",
    "            for thresh in np.unique(xs):\n",
    "                left = xs < thresh\n",
    "                right = ~left\n",
    "\n",
    "                if left.sum() == 0 or right.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # G_L, H_L\n",
    "                GL = np.sum(g[left])\n",
    "                HL = np.sum(h[left])\n",
    "\n",
    "                # G_R, H_R\n",
    "                GR = np.sum(g[right])\n",
    "                HR = np.sum(h[right])\n",
    "\n",
    "                # Gain (XGBoost original formula)\n",
    "                gain = 0.5 * (\n",
    "                    (GL**2) / (HL + self.lambda_) +\n",
    "                    (GR**2) / (HR + self.lambda_) -\n",
    "                    ( (GL+GR)**2 ) / ( (HL+HR) + self.lambda_)\n",
    "                )\n",
    "\n",
    "                print(f\"  Test split: x[{feature}] < {thresh}, gain={gain:.6f}\")\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_thresh = thresh\n",
    "                    best_masks = (left, right)\n",
    "\n",
    "        # If no positive gain → leaf\n",
    "        if best_feature is None:\n",
    "            leaf_value = -np.sum(g) / (np.sum(h) + self.lambda_)\n",
    "            print(f\"  No positive gain → Leaf={leaf_value:.4f}\")\n",
    "            return self.Node(is_leaf=True, value=leaf_value)\n",
    "\n",
    "        print(f\"\\n  >>> Best split: feature={best_feature}, thresh={best_thresh}, gain={best_gain:.6f}\")\n",
    "\n",
    "        left_mask, right_mask = best_masks\n",
    "\n",
    "        left_child = self._build_tree(X[left_mask],\n",
    "                                      g[left_mask],\n",
    "                                      h[left_mask],\n",
    "                                      depth + 1)\n",
    "\n",
    "        right_child = self._build_tree(X[right_mask],\n",
    "                                       g[right_mask],\n",
    "                                       h[right_mask],\n",
    "                                       depth + 1)\n",
    "\n",
    "        return self.Node(\n",
    "            is_leaf=False,\n",
    "            feature=best_feature,\n",
    "            thresh=best_thresh,\n",
    "            left=left_child,\n",
    "            right=right_child\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    #          Predict for one sample\n",
    "    # ---------------------------------------------------------\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.value\n",
    "        if x[node.feature] < node.thresh:\n",
    "            return self._predict_one(x, node.left)\n",
    "        return self._predict_one(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e8c2629-c156-44ba-91f3-b443150b5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostClassifierDebug:\n",
    "\n",
    "    def __init__(self, T=10, lr=0.3, max_depth=3, lambda_=1.0, min_child_weight=1.0):\n",
    "        self.T = T\n",
    "        self.lr = lr\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.trees = []\n",
    "        self.F0 = 0  # initial logits\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # initial probability\n",
    "        p0 = np.mean(y)\n",
    "        self.F0 = np.log(p0 / (1 - p0))\n",
    "        F = np.ones(len(y)) * self.F0\n",
    "\n",
    "        print(f\"\\n========= XGBoost Training Start =========\")\n",
    "        print(f\"Initial F0 = {self.F0:.4f}\\n\")\n",
    "\n",
    "        for t in range(self.T):\n",
    "            print(f\"\\n=== Iteration {t+1} ===\")\n",
    "\n",
    "            # probabilities\n",
    "            p = sigmoid(F)\n",
    "\n",
    "            # 1st order gradient (derivative of logloss)\n",
    "            g = p - y\n",
    "\n",
    "            # 2nd order gradient (Hessian)\n",
    "            h = p * (1 - p)\n",
    "\n",
    "            print(\"g:\", np.round(g, 4))\n",
    "            print(\"h:\", np.round(h, 4))\n",
    "\n",
    "            # train tree on (g, h)\n",
    "            tree = XGBTree(\n",
    "                max_depth=self.max_depth,\n",
    "                lambda_=self.lambda_,\n",
    "                min_child_weight=self.min_child_weight\n",
    "            )\n",
    "            tree.fit(X, g, h)\n",
    "\n",
    "            # update logits\n",
    "            pred = tree.predict(X)\n",
    "            print(\"Tree predictions:\", np.round(pred, 4))\n",
    "\n",
    "            F += self.lr * pred\n",
    "            print(\"Neue Logits F:\", np.round(F, 4))\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        print(\"\\n========= Training fertig =========\\n\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        F = np.ones(len(X)) * self.F0\n",
    "        for tree in self.trees:\n",
    "            F += self.lr * tree.predict(X)\n",
    "        return sigmoid(F)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11fe3789-f8a1-4bdf-8476-97e33c93edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= XGBoost Training Start =========\n",
      "Initial F0 = -0.6931\n",
      "\n",
      "\n",
      "=== Iteration 1 ===\n",
      "g: [ 0.3333  0.3333  0.3333 -0.6667  0.3333 -0.6667]\n",
      "h: [0.2222 0.2222 0.2222 0.2222 0.2222 0.2222]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.143541\n",
      "  Test split: x[0] < 3, gain=0.542986\n",
      "  Test split: x[0] < 4, gain=1.200000\n",
      "  Test split: x[0] < 5, gain=0.135747\n",
      "  Test split: x[0] < 6, gain=0.574163\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=1.200000\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=-0.6000\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=0.6000\n",
      "Tree predictions: [-0.6 -0.6 -0.6  0.6  0.6  0.6]\n",
      "Neue Logits F: [-0.9331 -0.9331 -0.9331 -0.4531 -0.4531 -0.4531]\n",
      "\n",
      "=== Iteration 2 ===\n",
      "g: [ 0.2823  0.2823  0.2823 -0.6114  0.3886 -0.6114]\n",
      "h: [0.2026 0.2026 0.2026 0.2376 0.2376 0.2376]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.100506\n",
      "  Test split: x[0] < 3, gain=0.385772\n",
      "  Test split: x[0] < 4, gain=0.852243\n",
      "  Test split: x[0] < 5, gain=0.063619\n",
      "  Test split: x[0] < 6, gain=0.488947\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.852243\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=-0.5267\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=0.4870\n",
      "Tree predictions: [-0.5267 -0.5267 -0.5267  0.487   0.487   0.487 ]\n",
      "Neue Logits F: [-1.1438 -1.1438 -1.1438 -0.2583 -0.2583 -0.2583]\n",
      "\n",
      "=== Iteration 3 ===\n",
      "g: [ 0.2416  0.2416  0.2416 -0.5642  0.4358 -0.5642]\n",
      "h: [0.1832 0.1832 0.1832 0.2459 0.2459 0.2459]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.069735\n",
      "  Test split: x[0] < 3, gain=0.276358\n",
      "  Test split: x[0] < 4, gain=0.614713\n",
      "  Test split: x[0] < 5, gain=0.024977\n",
      "  Test split: x[0] < 6, gain=0.429305\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.614713\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=-0.4677\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=0.3986\n",
      "Tree predictions: [-0.4677 -0.4677 -0.4677  0.3986  0.3986  0.3986]\n",
      "Neue Logits F: [-1.3309 -1.3309 -1.3309 -0.0989 -0.0989 -0.0989]\n",
      "\n",
      "=== Iteration 4 ===\n",
      "g: [ 0.209   0.209   0.209  -0.5247  0.4753 -0.5247]\n",
      "h: [0.1653 0.1653 0.1653 0.2494 0.2494 0.2494]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.047959\n",
      "  Test split: x[0] < 3, gain=0.199728\n",
      "  Test split: x[0] < 4, gain=0.450096\n",
      "  Test split: x[0] < 5, gain=0.006379\n",
      "  Test split: x[0] < 6, gain=0.386368\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.450096\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=-0.4191\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=0.3284\n",
      "Tree predictions: [-0.4191 -0.4191 -0.4191  0.3284  0.3284  0.3284]\n",
      "Neue Logits F: [-1.4986 -1.4986 -1.4986  0.0325  0.0325  0.0325]\n",
      "\n",
      "=== Iteration 5 ===\n",
      "g: [ 0.1826  0.1826  0.1826 -0.4919  0.5081 -0.4919]\n",
      "h: [0.1493 0.1493 0.1493 0.2499 0.2499 0.2499]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.032594\n",
      "  Test split: x[0] < 3, gain=0.145580\n",
      "  Test split: x[0] < 4, gain=0.334264\n",
      "  Test split: x[0] < 5, gain=-0.000352\n",
      "  Test split: x[0] < 6, gain=0.354595\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.354595\n",
      "\n",
      "--- Build node: depth=2, n=5 ---\n",
      "  Leaf: value=-0.2896\n",
      "\n",
      "--- Build node: depth=2, n=1 ---\n",
      "  Leaf: value=0.3935\n",
      "Tree predictions: [-0.2896 -0.2896 -0.2896 -0.2896 -0.2896  0.3935]\n",
      "Neue Logits F: [-1.6144 -1.6144 -1.6144 -0.0834 -0.0834  0.1899]\n",
      "\n",
      "=== Iteration 6 ===\n",
      "g: [ 0.166   0.166   0.166  -0.5208  0.4792 -0.4527]\n",
      "h: [0.1384 0.1384 0.1384 0.2496 0.2496 0.2478]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.037222\n",
      "  Test split: x[0] < 3, gain=0.143481\n",
      "  Test split: x[0] < 4, gain=0.315058\n",
      "  Test split: x[0] < 5, gain=0.000778\n",
      "  Test split: x[0] < 6, gain=0.272953\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.315058\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=-0.3518\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=0.2830\n",
      "Tree predictions: [-0.3518 -0.3518 -0.3518  0.283   0.283   0.283 ]\n",
      "Neue Logits F: [-1.7552 -1.7552 -1.7552  0.0298  0.0298  0.3031]\n",
      "\n",
      "=== Iteration 7 ===\n",
      "g: [ 0.1474  0.1474  0.1474 -0.4925  0.5075 -0.4248]\n",
      "h: [0.1257 0.1257 0.1257 0.2499 0.2499 0.2443]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.025448\n",
      "  Test split: x[0] < 3, gain=0.105807\n",
      "  Test split: x[0] < 4, gain=0.237833\n",
      "  Test split: x[0] < 5, gain=0.005638\n",
      "  Test split: x[0] < 6, gain=0.255851\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.255851\n",
      "\n",
      "--- Build node: depth=2, n=5 ---\n",
      "  Leaf: value=-0.2435\n",
      "\n",
      "--- Build node: depth=2, n=1 ---\n",
      "  Leaf: value=0.3414\n",
      "Tree predictions: [-0.2435 -0.2435 -0.2435 -0.2435 -0.2435  0.3414]\n",
      "Neue Logits F: [-1.8526 -1.8526 -1.8526 -0.0676 -0.0676  0.4396]\n",
      "\n",
      "=== Iteration 8 ===\n",
      "g: [ 0.1356  0.1356  0.1356 -0.5169  0.4831 -0.3918]\n",
      "h: [0.1172 0.1172 0.1172 0.2497 0.2497 0.2383]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.028380\n",
      "  Test split: x[0] < 3, gain=0.104740\n",
      "  Test split: x[0] < 4, gain=0.226459\n",
      "  Test split: x[0] < 5, gain=0.013010\n",
      "  Test split: x[0] < 6, gain=0.198945\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.226459\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=-0.3009\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=0.2449\n",
      "Tree predictions: [-0.3009 -0.3009 -0.3009  0.2449  0.2449  0.2449]\n",
      "Neue Logits F: [-1.9729 -1.9729 -1.9729  0.0304  0.0304  0.5376]\n",
      "\n",
      "=== Iteration 9 ===\n",
      "g: [ 0.1221  0.1221  0.1221 -0.4924  0.5076 -0.3687]\n",
      "h: [0.1072 0.1072 0.1072 0.2499 0.2499 0.2328]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.019530\n",
      "  Test split: x[0] < 3, gain=0.078134\n",
      "  Test split: x[0] < 4, gain=0.173555\n",
      "  Test split: x[0] < 5, gain=0.023058\n",
      "  Test split: x[0] < 6, gain=0.190085\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.190085\n",
      "\n",
      "--- Build node: depth=2, n=5 ---\n",
      "  Leaf: value=-0.2094\n",
      "\n",
      "--- Build node: depth=2, n=1 ---\n",
      "  Leaf: value=0.2991\n",
      "Tree predictions: [-0.2094 -0.2094 -0.2094 -0.2094 -0.2094  0.2991]\n",
      "Neue Logits F: [-2.0567 -2.0567 -2.0567 -0.0534 -0.0534  0.6573]\n",
      "\n",
      "=== Iteration 10 ===\n",
      "g: [ 0.1134  0.1134  0.1134 -0.5133  0.4867 -0.3414]\n",
      "h: [0.1005 0.1005 0.1005 0.2498 0.2498 0.2248]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.021664\n",
      "  Test split: x[0] < 3, gain=0.077963\n",
      "  Test split: x[0] < 4, gain=0.167049\n",
      "  Test split: x[0] < 5, gain=0.033272\n",
      "  Test split: x[0] < 6, gain=0.149294\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.167049\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=-0.2613\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Leaf: value=0.2134\n",
      "Tree predictions: [-0.2613 -0.2613 -0.2613  0.2134  0.2134  0.2134]\n",
      "Neue Logits F: [-2.1612 -2.1612 -2.1612  0.032   0.032   0.7426]\n",
      "\n",
      "========= Training fertig =========\n",
      "\n",
      "Pred: [0 0 0 1 1 1]\n",
      "True: [0 0 0 1 0 1]\n",
      "Proba: [0.103 0.103 0.103 0.508 0.508 0.678]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[2],[3],[4],[5],[6]])\n",
    "y = np.array([0,0,0,1,0,1])\n",
    "\n",
    "xgb = XGBoostClassifierDebug(\n",
    "    T=10,\n",
    "    lr=0.4,\n",
    "    max_depth=3,\n",
    "    lambda_=1.0,\n",
    "    min_child_weight=1.0\n",
    ")\n",
    "\n",
    "xgb.fit(X, y)\n",
    "\n",
    "print(\"Pred:\", xgb.predict(X))\n",
    "print(\"True:\", y)\n",
    "print(\"Proba:\", np.round(xgb.predict_proba(X), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85c8b9-c80f-480e-b540-61aa800b37b6",
   "metadata": {},
   "source": [
    "## Warum unser selbst gebautes XGBoost mit 100 Iterationen das einfache Beispiel nicht perfekt löst\n",
    "\n",
    "Auf den ersten Blick wirkt es seltsam: Wir haben ein sehr kleines, übersichtliches Datenset (nur 6 Punkte, 1 Feature), 100 Boosting-Iterationen, und trotzdem klassifiziert unser XGBoost-Klon einen Punkt weiterhin falsch. Das ist ein guter Anlass, genauer hinzuschauen, was im Inneren des Algorithmus passiert.\n",
    "\n",
    "### 1. Die Implementierung ist prinzipiell korrekt – aber zu „streng“ für Minidaten\n",
    "\n",
    "Die Grundidee unseres Codes stimmt:\n",
    "\n",
    "- Wir arbeiten mit Log-Loss und Logits \\(F(x)\\).\n",
    "- Wir berechnen erste und zweite Ableitung der Loss:\n",
    "  - Gradient \\(g_i = p_i - y_i\\)\n",
    "  - Hessian \\(h_i = p_i(1 - p_i)\\)\n",
    "- Wir lernen Bäume, deren Leaf-Werte nach der XGBoost-Formel bestimmt werden:\n",
    "  \\[\n",
    "  w = -\\frac{\\sum g_i}{\\sum h_i + \\lambda}\n",
    "  \\]\n",
    "- Wir wählen Splits über einen Gain:\n",
    "  \\[\n",
    "  \\text{Gain} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda} \\right)\n",
    "  \\]\n",
    "\n",
    "Das ist inhaltlich XGBoost-Logik. Das Problem liegt nicht in der Theorie, sondern in der Kombination aus:\n",
    "\n",
    "- extrem kleinem Datensatz,\n",
    "- konserviven Default-Hyperparametern und\n",
    "- strengen Abbruchkriterien.\n",
    "\n",
    "### 2. Warum unsere Bäume kaum noch etwas lernen\n",
    "\n",
    "Entscheidend sind drei Punkte:\n",
    "\n",
    "1. **Gain-Schwelle**  \n",
    "   Wir initialisieren `best_gain` mit 0 und akzeptieren nur Splits, deren Gain > 0 ist.  \n",
    "   Bei nur 6 Datenpunkten sind die Gradienten- und Hessian-Summen sehr klein. Dadurch wird der berechnete Gain häufig nur minimal positiv oder sogar leicht negativ.  \n",
    "   Ergebnis: Oft findet der Baum keinen Split mit Gain > 0 und wird direkt zu einem Leaf.\n",
    "\n",
    "2. **L2-Regularisierung (\\(\\lambda\\))**  \n",
    "   In der Leaf-Formel taucht \\(\\lambda\\) im Nenner auf:\n",
    "   \\[\n",
    "   w = -\\frac{G}{H + \\lambda}\n",
    "   \\]\n",
    "   Wenn \\(H\\) (Summe der Hessians) klein ist und \\(\\lambda\\) relativ groß gewählt ist (z. B. 1.0), dann wird \\(w\\) sehr stark „klein gedrückt“.  \n",
    "   Die Bäume erzeugen dann Leaf-Werte, die nur winzige Updates machen. Selbst mit vielen Iterationen verschiebt sich \\(F(x)\\) nur langsam.\n",
    "\n",
    "3. **`min_child_weight` und kleine Leaves**  \n",
    "   XGBoost bricht Splits ab, wenn die Summe der Hessians in einem Kind-Knoten kleiner als `min_child_weight` ist. Bei 6 Datenpunkten und Log-Loss liegen die Hessians schnell im Bereich 0.1–0.25. Die Summe pro Leaf ist dann klein.  \n",
    "   Wenn `min_child_weight` zu hoch gewählt ist, werden viele Splits gar nicht mehr erlaubt, weil die Leaves als „zu klein / zu unsicher“ eingestuft werden.\n",
    "\n",
    "In Kombination führen diese Mechanismen dazu, dass die meisten Iterationen nur Bäume mit „fast nichts“ lernen:\n",
    "\n",
    "- Entweder sie werden sofort zu einem Leaf mit Wert nahe 0,\n",
    "- oder sie produzieren extrem kleine Leaf-Werte,\n",
    "- oder sie splitten kaum, weil `min_child_weight` den Split blockiert.\n",
    "\n",
    "Das erklärt den beobachteten Verlauf: Die Wahrscheinlichkeiten bewegen sich langsam, aber kommen nicht so klar über die 0.5-Schwelle, dass alle Punkte korrekt klassifiziert werden.\n",
    "\n",
    "### 3. Warum das bei echten XGBoost-Setups kaum auffällt\n",
    "\n",
    "Auf großen Datensätzen mit vielen Beobachtungen:\n",
    "\n",
    "- sind die Gradienten- und Hessian-Summen größer,\n",
    "- ist der Gain deutlich von 0 unterscheidbar,\n",
    "- wirken Regularisierung und `min_child_weight` als sinnvolle Stabilisierung.\n",
    "\n",
    "Auf Minidatensätzen hingegen dominiert die Regularisierung alles und die eingebauten „Sicherheitsmechanismen“ sorgen dafür, dass der Baum lieber gar nicht splittet, als einen potentiell instabilen Split mit wenig Daten zu machen.\n",
    "\n",
    "### 4. Was man ändern müsste, damit das Beispiel „sichtbar“ funktioniert\n",
    "\n",
    "Für didaktische Zwecke (kleines 1D-Spielbeispiel) kann man die Implementierung und Hyperparameter „entschärfen“, zum Beispiel:\n",
    "\n",
    "- `best_gain` nicht bei 0, sondern bei \\(-\\infty\\) initialisieren, damit immer irgendein Split gewählt wird.\n",
    "- \\(\\lambda\\) stark reduzieren oder auf 0 setzen, damit die Leaf-Werte nicht weggedämpft werden.\n",
    "- `min_child_weight` auf 0 setzen, damit auch kleine Leaves zugelassen werden.\n",
    "- Optional: Falls kein Split mit positivem Gain gefunden wird, einen einfachen Fallback-Split (z. B. Median-Split) verwenden, statt sofort ein Leaf zu erzeugen.\n",
    "\n",
    "Dann verhält sich der Algorithmus auf diesem Spielzeugbeispiel viel „aggressiver“ und man sieht, wie er die Struktur der 6 Punkte tatsächlich lernt.\n",
    "\n",
    "### 5. Zusammenfassung\n",
    "\n",
    "Die Implementierung zeigt sehr gut:\n",
    "\n",
    "- XGBoost ist nicht immer einfach gut – es ist ein Gradient-Boosting-Schema mit vielen Stabilitäts-Tricks.\n",
    "- Auf sehr kleinen Datensätzen können genau diese Tricks dazu führen, dass nichts mehr passiert.\n",
    "- Die Hyperparameter (`lambda`, `min_child_weight`, Tiefe, Gain-Schwelle) sind nicht nur Feintuning, sondern steuern direkt, ob das Modell überhaupt noch lernt.\n",
    "\n",
    "Das erklärt, warum unser selbstgeschriebener XGBoost mit 100 Iterationen auf 6 Punkten nicht perfekt wird – und welche Stellschrauben man (für Lernzwecke) lockern müsste, damit er es tut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6488c8cb-f249-4488-a6ec-ab740bfb94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBTree:\n",
    "    class Node:\n",
    "        def __init__(self, is_leaf=False, value=None,\n",
    "                     feature=None, thresh=None, left=None, right=None):\n",
    "            self.is_leaf = is_leaf\n",
    "            self.value = value\n",
    "            self.feature = feature\n",
    "            self.thresh = thresh\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "    def __init__(self, max_depth=3, lambda_=1.0, min_child_weight=1.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.root = None\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    #           FIT (Newton, Gain-basiert)\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, X, g, h):\n",
    "        X = np.array(X)\n",
    "        self.root = self._build_tree(X, g, h, depth=1)\n",
    "        return self\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    #         Bilde einen Baum rekursiv\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_tree(self, X, g, h, depth):\n",
    "\n",
    "        print(f\"\\n--- Build node: depth={depth}, n={len(X)} ---\")\n",
    "\n",
    "        # Leaf criteria\n",
    "        if depth > self.max_depth or len(X) <= 1 or np.sum(h) < self.min_child_weight:\n",
    "            leaf_value = -np.sum(g) / (np.sum(h) + self.lambda_)\n",
    "            print(f\"  Leaf: value={leaf_value:.4f}\")\n",
    "            return self.Node(is_leaf=True, value=leaf_value)\n",
    "\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "        best_masks = None\n",
    "\n",
    "        # Try all features + thresholds\n",
    "        for feature in range(X.shape[1]):\n",
    "            xs = X[:, feature]\n",
    "\n",
    "            for thresh in np.unique(xs):\n",
    "                left = xs < thresh\n",
    "                right = ~left\n",
    "\n",
    "                if left.sum() == 0 or right.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # G_L, H_L\n",
    "                GL = np.sum(g[left])\n",
    "                HL = np.sum(h[left])\n",
    "\n",
    "                # G_R, H_R\n",
    "                GR = np.sum(g[right])\n",
    "                HR = np.sum(h[right])\n",
    "\n",
    "                # Gain (XGBoost original formula)\n",
    "                gain = (\n",
    "                    (GL**2)/(HL + self.lambda_) +\n",
    "                    (GR**2)/(HR + self.lambda_) -\n",
    "                    ((GL+GR)**2)/( (HL+HR) + self.lambda_)\n",
    "                )\n",
    "\n",
    "                print(f\"  Test split: x[{feature}] < {thresh}, gain={gain:.6f}\")\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_thresh = thresh\n",
    "                    best_masks = (left, right)\n",
    "\n",
    "        # If no positive gain → leaf\n",
    "        if best_feature is None:\n",
    "            print(\"  Kein Split mit Gain > 0 — Fallback-Split wird verwendet.\")\n",
    "        \n",
    "            # Fallback: median-split auf Feature 0\n",
    "            best_feature = 0\n",
    "            best_thresh = np.median(X[:, 0])\n",
    "        \n",
    "            left = X[:,0] < best_thresh\n",
    "            right = ~left\n",
    "        \n",
    "            # Falls sogar das failt → Leaf\n",
    "            if left.sum() == 0 or right.sum() == 0:\n",
    "                leaf_value = -np.sum(g) / (np.sum(h) + self.lambda_)\n",
    "                print(f\"  Selbst Fallback nicht möglich → Leaf={leaf_value:.4f}\")\n",
    "                return self.Node(is_leaf=True, value=leaf_value)\n",
    "        \n",
    "            best_masks = (left, right)\n",
    "        \n",
    "            print(f\"  Fallback-Split: feature={best_feature}, thresh={best_thresh}\")\n",
    "\n",
    "        print(f\"\\n  >>> Best split: feature={best_feature}, thresh={best_thresh}, gain={best_gain:.6f}\")\n",
    "\n",
    "        left_mask, right_mask = best_masks\n",
    "\n",
    "        left_child = self._build_tree(X[left_mask],\n",
    "                                      g[left_mask],\n",
    "                                      h[left_mask],\n",
    "                                      depth + 1)\n",
    "\n",
    "        right_child = self._build_tree(X[right_mask],\n",
    "                                       g[right_mask],\n",
    "                                       h[right_mask],\n",
    "                                       depth + 1)\n",
    "\n",
    "        return self.Node(\n",
    "            is_leaf=False,\n",
    "            feature=best_feature,\n",
    "            thresh=best_thresh,\n",
    "            left=left_child,\n",
    "            right=right_child\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    #          Predict for one sample\n",
    "    # ---------------------------------------------------------\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.value\n",
    "        if x[node.feature] < node.thresh:\n",
    "            return self._predict_one(x, node.left)\n",
    "        return self._predict_one(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4746221-b4d2-4f42-9921-a5a6cfea29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= XGBoost Training Start =========\n",
      "Initial F0 = -0.6931\n",
      "\n",
      "\n",
      "=== Iteration 1 ===\n",
      "g: [ 0.3333  0.3333  0.3333 -0.6667  0.3333 -0.6667]\n",
      "h: [0.2222 0.2222 0.2222 0.2222 0.2222 0.2222]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.600000\n",
      "  Test split: x[0] < 3, gain=1.500000\n",
      "  Test split: x[0] < 4, gain=3.000000\n",
      "  Test split: x[0] < 5, gain=0.375000\n",
      "  Test split: x[0] < 6, gain=2.400000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=3.000000\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.5000\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.5000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.5000\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.750000\n",
      "  Test split: x[0] < 6, gain=0.750000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.750000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=3.0000\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=2.250000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=2.250000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.5000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=3.0000\n",
      "Tree predictions: [-1.5 -1.5 -1.5  3.  -1.5  3. ]\n",
      "Neue Logits F: [-1.2931 -1.2931 -1.2931  0.5069 -1.2931  0.5069]\n",
      "\n",
      "=== Iteration 2 ===\n",
      "g: [ 0.2153  0.2153  0.2153 -0.3759  0.2153 -0.3759]\n",
      "h: [0.169  0.169  0.169  0.2346 0.169  0.2346]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.275440\n",
      "  Test split: x[0] < 3, gain=0.666196\n",
      "  Test split: x[0] < 4, gain=1.263859\n",
      "  Test split: x[0] < 5, gain=0.151803\n",
      "  Test split: x[0] < 6, gain=0.850671\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=1.263859\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=-0.000000\n",
      "  Test split: x[0] < 3, gain=-0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=-0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.2744\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.2744\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.2744\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.215211\n",
      "  Test split: x[0] < 6, gain=0.215211\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.215211\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.6024\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.812874\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.812874\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.2744\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.6024\n",
      "Tree predictions: [-1.2744 -1.2744 -1.2744  1.6024 -1.2744  1.6024]\n",
      "Neue Logits F: [-1.8029 -1.8029 -1.8029  1.1478 -1.8029  1.1478]\n",
      "\n",
      "=== Iteration 3 ===\n",
      "g: [ 0.1415  0.1415  0.1415 -0.2409  0.1415 -0.2409]\n",
      "h: [0.1215 0.1215 0.1215 0.1829 0.1215 0.1829]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.160987\n",
      "  Test split: x[0] < 3, gain=0.386231\n",
      "  Test split: x[0] < 4, gain=0.723798\n",
      "  Test split: x[0] < 5, gain=0.085727\n",
      "  Test split: x[0] < 6, gain=0.467042\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.723798\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.1648\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.1648\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.1648\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.112123\n",
      "  Test split: x[0] < 6, gain=0.112123\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.112123\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.3173\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.449690\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.449690\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.1648\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.3173\n",
      "Tree predictions: [-1.1648 -1.1648 -1.1648  1.3173 -1.1648  1.3173]\n",
      "Neue Logits F: [-2.2688 -2.2688 -2.2688  1.6747 -2.2688  1.6747]\n",
      "\n",
      "=== Iteration 4 ===\n",
      "g: [ 0.0937  0.0937  0.0937 -0.1578  0.0937 -0.1578]\n",
      "h: [0.085  0.085  0.085  0.1329 0.085  0.1329]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.099883\n",
      "  Test split: x[0] < 3, gain=0.238717\n",
      "  Test split: x[0] < 4, gain=0.444802\n",
      "  Test split: x[0] < 5, gain=0.052300\n",
      "  Test split: x[0] < 6, gain=0.281298\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.444802\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.1034\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.1034\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.1034\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.065868\n",
      "  Test split: x[0] < 6, gain=0.065868\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.065868\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.1874\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.271954\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.271954\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.1034\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.1874\n",
      "Tree predictions: [-1.1034 -1.1034 -1.1034  1.1874 -1.1034  1.1874]\n",
      "Neue Logits F: [-2.7102 -2.7102 -2.7102  2.1497 -2.7102  2.1497]\n",
      "\n",
      "=== Iteration 5 ===\n",
      "g: [ 0.0624  0.0624  0.0624 -0.1044  0.0624 -0.1044]\n",
      "h: [0.0585 0.0585 0.0585 0.0935 0.0585 0.0935]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.063860\n",
      "  Test split: x[0] < 3, gain=0.152299\n",
      "  Test split: x[0] < 4, gain=0.282888\n",
      "  Test split: x[0] < 5, gain=0.033121\n",
      "  Test split: x[0] < 6, gain=0.176907\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.282888\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.0665\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0665\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0665\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.040854\n",
      "  Test split: x[0] < 6, gain=0.040854\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.040854\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.1165\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.171442\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.171442\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0665\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.1165\n",
      "Tree predictions: [-1.0665 -1.0665 -1.0665  1.1165 -1.0665  1.1165]\n",
      "Neue Logits F: [-3.1368 -3.1368 -3.1368  2.5963 -3.1368  2.5963]\n",
      "\n",
      "=== Iteration 6 ===\n",
      "g: [ 0.0416  0.0416  0.0416 -0.0694  0.0416 -0.0694]\n",
      "h: [0.0399 0.0399 0.0399 0.0646 0.0399 0.0646]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.041541\n",
      "  Test split: x[0] < 3, gain=0.098943\n",
      "  Test split: x[0] < 4, gain=0.183437\n",
      "  Test split: x[0] < 5, gain=0.021422\n",
      "  Test split: x[0] < 6, gain=0.113946\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.183437\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.0434\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0434\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0434\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.026096\n",
      "  Test split: x[0] < 6, gain=0.026096\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.026096\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.0745\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.110590\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.110590\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0434\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.0745\n",
      "Tree predictions: [-1.0434 -1.0434 -1.0434  1.0745 -1.0434  1.0745]\n",
      "Neue Logits F: [-3.5542 -3.5542 -3.5542  3.0261 -3.5542  3.0261]\n",
      "\n",
      "=== Iteration 7 ===\n",
      "g: [ 0.0278  0.0278  0.0278 -0.0463  0.0278 -0.0463]\n",
      "h: [0.027  0.027  0.027  0.0441 0.027  0.0441]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.027309\n",
      "  Test split: x[0] < 3, gain=0.064994\n",
      "  Test split: x[0] < 4, gain=0.120356\n",
      "  Test split: x[0] < 5, gain=0.014032\n",
      "  Test split: x[0] < 6, gain=0.074448\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.120356\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.0286\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0286\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0286\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.016962\n",
      "  Test split: x[0] < 6, gain=0.016962\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.016962\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.0485\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.072324\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.072324\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0286\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.0485\n",
      "Tree predictions: [-1.0286 -1.0286 -1.0286  1.0485 -1.0286  1.0485]\n",
      "Neue Logits F: [-3.9656 -3.9656 -3.9656  3.4455 -3.9656  3.4455]\n",
      "\n",
      "=== Iteration 8 ===\n",
      "g: [ 0.0186  0.0186  0.0186 -0.0309  0.0186 -0.0309]\n",
      "h: [0.0183 0.0183 0.0183 0.0299 0.0183 0.0299]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.018074\n",
      "  Test split: x[0] < 3, gain=0.042993\n",
      "  Test split: x[0] < 4, gain=0.079554\n",
      "  Test split: x[0] < 5, gain=0.009265\n",
      "  Test split: x[0] < 6, gain=0.049077\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.079554\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.0190\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0190\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0190\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.011145\n",
      "  Test split: x[0] < 6, gain=0.011145\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.011145\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.0319\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.047706\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.047706\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0190\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.0319\n",
      "Tree predictions: [-1.019  -1.019  -1.019   1.0319 -1.019   1.0319]\n",
      "Neue Logits F: [-4.3732 -4.3732 -4.3732  3.8583 -4.3732  3.8583]\n",
      "\n",
      "=== Iteration 9 ===\n",
      "g: [ 0.0125  0.0125  0.0125 -0.0207  0.0125 -0.0207]\n",
      "h: [0.0123 0.0123 0.0123 0.0202 0.0123 0.0202]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.012013\n",
      "  Test split: x[0] < 3, gain=0.028567\n",
      "  Test split: x[0] < 4, gain=0.052836\n",
      "  Test split: x[0] < 5, gain=0.006149\n",
      "  Test split: x[0] < 6, gain=0.032537\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.052836\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=-0.000000\n",
      "  Test split: x[0] < 3, gain=-0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=-0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.0126\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0126\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0126\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.007373\n",
      "  Test split: x[0] < 6, gain=0.007373\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.007373\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.0211\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.031641\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.031641\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0126\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.0211\n",
      "Tree predictions: [-1.0126 -1.0126 -1.0126  1.0211 -1.0126  1.0211]\n",
      "Neue Logits F: [-4.7783 -4.7783 -4.7783  4.2667 -4.7783  4.2667]\n",
      "\n",
      "=== Iteration 10 ===\n",
      "g: [ 0.0083  0.0083  0.0083 -0.0138  0.0083 -0.0138]\n",
      "h: [0.0083 0.0083 0.0083 0.0136 0.0083 0.0136]\n",
      "\n",
      "--- Build node: depth=1, n=6 ---\n",
      "  Test split: x[0] < 2, gain=0.008008\n",
      "  Test split: x[0] < 3, gain=0.019038\n",
      "  Test split: x[0] < 4, gain=0.035200\n",
      "  Test split: x[0] < 5, gain=0.004095\n",
      "  Test split: x[0] < 6, gain=0.021652\n",
      "\n",
      "  >>> Best split: feature=0, thresh=4, gain=0.035200\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 2, gain=0.000000\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=2, gain=0.000000\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=-1.0084\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 3, gain=0.000000\n",
      "\n",
      "  >>> Best split: feature=0, thresh=3, gain=0.000000\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0084\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0084\n",
      "\n",
      "--- Build node: depth=2, n=3 ---\n",
      "  Test split: x[0] < 5, gain=0.004899\n",
      "  Test split: x[0] < 6, gain=0.004899\n",
      "\n",
      "  >>> Best split: feature=0, thresh=5, gain=0.004899\n",
      "\n",
      "--- Build node: depth=3, n=1 ---\n",
      "  Leaf: value=1.0140\n",
      "\n",
      "--- Build node: depth=3, n=2 ---\n",
      "  Test split: x[0] < 6, gain=0.021061\n",
      "\n",
      "  >>> Best split: feature=0, thresh=6, gain=0.021061\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=-1.0084\n",
      "\n",
      "--- Build node: depth=4, n=1 ---\n",
      "  Leaf: value=1.0140\n",
      "Tree predictions: [-1.0084 -1.0084 -1.0084  1.014  -1.0084  1.014 ]\n",
      "Neue Logits F: [-5.1816 -5.1816 -5.1816  4.6723 -5.1816  4.6723]\n",
      "\n",
      "========= Training fertig =========\n",
      "\n",
      "Pred: [0 0 0 1 0 1]\n",
      "True: [0 0 0 1 0 1]\n",
      "Proba: [0.006 0.006 0.006 0.991 0.006 0.991]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[2],[3],[4],[5],[6]])\n",
    "y = np.array([0,0,0,1,0,1])\n",
    "\n",
    "xgb = XGBoostClassifierDebug(\n",
    "    T=10,\n",
    "    lr=0.4,\n",
    "    max_depth=3,\n",
    "    lambda_=0.0,\n",
    "    min_child_weight=0.0\n",
    ")\n",
    "\n",
    "xgb.fit(X, y)\n",
    "\n",
    "print(\"Pred:\", xgb.predict(X))\n",
    "print(\"True:\", y)\n",
    "print(\"Proba:\", np.round(xgb.predict_proba(X), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdb7ec-137b-4ae3-8efe-409850185506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml25)",
   "language": "python",
   "name": "aml25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

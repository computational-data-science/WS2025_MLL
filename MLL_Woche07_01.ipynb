{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19dfab3d-57b7-4b6b-ad60-24a532ece174",
   "metadata": {},
   "source": [
    "# Boosting – Grundidee\n",
    "\n",
    "Boosting erzeugt ein starkes Modell, indem es **viele schwache Modelle** (meist flache Bäume)\n",
    "**nacheinander** trainiert.\n",
    "\n",
    "Das Wichtige:  \n",
    "Jeder neue Baum **korrigiert gezielt die Fehler des bisherigen Modells**.\n",
    "\n",
    "\n",
    "## Mathematische Sicht\n",
    "\n",
    "Wir konstruieren ein Modell als additive Funktion:\n",
    "\n",
    "$$\n",
    "F_T(x) = \\sum_{t=1}^T \\alpha_t \\, h_t(x)\n",
    "$$\n",
    "\n",
    "wobei  \n",
    "- $ h_t(x) $ = schwacher Lerner (z. B. Decision Stump)  \n",
    "- $ \\alpha_t $ = Gewicht des t-ten Baums  \n",
    "- $ T $ = Anzahl der Boosting-Schritte  \n",
    "\n",
    "Alle drei Boosting-Methoden unterscheiden sich nur darin:  \n",
    " **Wie werden $ h_t(x) $ ausgewählt?**  \n",
    " **Wie werden Fehler gewichtet?**  \n",
    " **Wie wird $ \\alpha_t $ berechnet?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14c63c-8152-46f7-b61d-a637d2f04912",
   "metadata": {},
   "source": [
    "# 1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "## Grundidee\n",
    "AdaBoost passt im Trainingsprozess die **Gewichte der Datenpunkte** an:\n",
    "- Falsch klassifizierte Beobachtungen werden wichtiger.\n",
    "- Richtig klassifizierte werden weniger wichtig.\n",
    "\n",
    "Damit konzentriert sich jeder neue Baum immer mehr auf die schwierigen Beispiele.\n",
    "\n",
    "\n",
    "# Formale Definition\n",
    "\n",
    "Wir starten mit Gewichten\n",
    "\n",
    "$$\n",
    "w_i^{(1)} = \\frac{1}{n}\n",
    "$$\n",
    "\n",
    "In jedem Boosting-Schritt trainieren wir einen Klassifikator $ h_t(x) $, typischerweise:\n",
    "\n",
    "- ein **Decision Stump**, also ein Baum mit Tiefe 1  \n",
    "- Vorhersagewerte $ h_t(x) \\in \\{-1, +1\\} $\n",
    "\n",
    "\n",
    "## 1. Gewichteter Fehler\n",
    "\n",
    "$$\n",
    "\\text{err}_t = \\sum_{i=1}^{n} w_i^{(t)} \\, \\mathbf{1}(h_t(x_i)\\neq y_i)\n",
    "$$\n",
    "\n",
    "\n",
    "## 2. Modellgewicht (Stärke des Baums)\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\text{err}_t}{\\text{err}_t}\\right)\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- guter Baum $\\Rightarrow \\text{err}_t$ klein → $\\alpha_t$ groß  \n",
    "- schlechter Baum → $\\alpha_t$ klein oder negativ\n",
    "\n",
    "\n",
    "## 3. Update der Punktgewichte\n",
    "\n",
    "$$\n",
    "w_i^{(t+1)} = w_i^{(t)} \\, \\exp\\left( -\\alpha_t \\, y_i \\, h_t(x_i) \\right)\n",
    "$$\n",
    "\n",
    "Da $ y_i, h_t(x_i) \\in \\{-1,+1\\} $:\n",
    "\n",
    "- Wenn **richtig klassifiziert** → exponent = −α → Gewicht sinkt  \n",
    "- Wenn **falsch** → exponent = +α → Gewicht steigt  \n",
    "\n",
    "Am Ende: Normieren auf Summe 1.\n",
    "\n",
    "\n",
    "## Finale Vorhersage\n",
    "\n",
    "$$\n",
    "F(x) = \\mathrm{sign}\\left( \\sum_{t=1}^T \\alpha_t h_t(x) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### Hauptpunkte\n",
    "1. AdaBoost optimiert direkt die **exponentielle Loss**  \n",
    "   $$\n",
    "   L = \\sum_i \\exp(-y_i F(x_i))\n",
    "   $$\n",
    "2. Der Algorithmus “schaut” immer wieder auf dieselben Fehler.\n",
    "3. Viele schwache Stumps → komplexe, stückweise lineare Entscheidungsgre​nze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c5a6d-05d3-4fa2-b96b-5a649292d806",
   "metadata": {},
   "source": [
    "# AdaBoost (Pseudocode)\n",
    "initialize w = [1/n] * n\n",
    "\n",
    "for t in range(1, T+1):\n",
    "\n",
    "    # 1. Train weak learner on weighted data\n",
    "    h_t = train_stump(X, y, sample_weight=w)\n",
    "    \n",
    "    # 2. Weighted error\n",
    "    err_t = sum(w[i] for i in range(n) if h_t(x_i) != y[i])\n",
    "    \n",
    "    # 3. Compute tree strength\n",
    "    alpha_t = 0.5 * np.log((1 - err_t) / err_t)\n",
    "    \n",
    "    # 4. Update sample weights\n",
    "    for i in range(n):\n",
    "        w[i] = w[i] * np.exp(-alpha_t * y[i] * h_t(x_i))\n",
    "    \n",
    "    # 5. Normalize\n",
    "    w = w / sum(w)\n",
    "\n",
    "# Final model:\n",
    "# F(x) = sign(sum(alpha_t * h_t(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df11f8c-b8b5-41b6-913c-2bb6ef31c242",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47eb7e-0ff2-43e2-ab1b-e33ff9da675b",
   "metadata": {},
   "source": [
    "# 2. Gradient Boosting – Theorie\n",
    "\n",
    "Gradient Boosting betrachtet Boosting als **Gradientenabstieg im Funktionsraum**.\n",
    "\n",
    "Wir versuchen eine Funktion\n",
    "\n",
    "$$\n",
    "F(x)\n",
    "$$\n",
    "\n",
    "zu finden, die eine Loss-Funktion minimiert:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} \\ell(y_i, F(x_i))\n",
    "$$\n",
    "\n",
    "(z. B. quadratische Loss, Log-Loss, Huber, etc.)\n",
    "\n",
    "\n",
    "# Kernschritt:\n",
    "In jedem Boosting-Schritt wird ein Baum auf den **negativen Gradienten** der Loss-Funktion trainiert.\n",
    "\n",
    "\n",
    "## Beispiel: Regression mit quadratischer Loss\n",
    "$$\n",
    "\\ell(y, F) = \\frac12 (y - F)^2\n",
    "$$\n",
    "\n",
    "Gradient der Loss:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial F} = F(x_i)-y_i\n",
    "$$\n",
    "\n",
    "Negativer Gradient (das, was wir fitten!):\n",
    "\n",
    "$$\n",
    "r_i^{(t)} = y_i - F_{t-1}(x_i)\n",
    "$$\n",
    "\n",
    "\n",
    "# Algorithmus\n",
    "\n",
    "**Initialisierung:**\n",
    "\n",
    "$$\n",
    "F_0(x) = \\arg \\min_c \\sum_i \\ell(y_i, c)\n",
    "$$\n",
    "\n",
    "Für quadratische Loss: einfach Mittelwert von y.\n",
    "\n",
    "\n",
    "**Jeder Boosting-Schritt:**\n",
    "\n",
    "1. Kompute Residuen  \n",
    "$$\n",
    "r_i^{(t)} = - \\left[\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F} \\right]_{F=F_{t-1}}\n",
    "$$\n",
    "\n",
    "2. Fit eines kleinen Regressionsbaums \\(h_t(x)\\) auf die Residuen\n",
    "\n",
    "3. Optional pro Blatt eine optimale Schrittweite \\(\\gamma_{tj}\\):  \n",
    "$$\n",
    "\\gamma_{tj} = \\arg\\min_\\gamma \\sum_{x_i \\in \\text{leaf }j} \\ell\\left(y_i, F_{t-1}(x_i) + \\gamma\\right)\n",
    "$$\n",
    "\n",
    "4. Update des Modells  \n",
    "$$\n",
    "F_t(x) = F_{t-1}(x) + \\eta \\, h_t(x)\n",
    "$$\n",
    "\n",
    "\n",
    "# Finale Interpretation\n",
    "\n",
    "Gradient Boosting repariert Fehler über **Gradienten der Loss**, nicht über Punktgewichte.\n",
    "\n",
    "Es ist wesentlich allgemeiner und flexibler als AdaBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553939f-b969-4984-a74a-caff68a3ef9b",
   "metadata": {},
   "source": [
    "# Gradient Boosting (konzeptioneller Pseudocode)\n",
    "\n",
    "# Step 1: initial model\n",
    "F = lambda x: np.mean(y)\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    \n",
    "    # 2. Compute negative gradient (residuals)\n",
    "    residuals = [y[i] - F(x_i) for i in range(n)]\n",
    "    \n",
    "    # 3. Fit regression tree to residuals\n",
    "    h_t = fit_tree(X, residuals, max_depth=3)\n",
    "    \n",
    "    # 4. Update model\n",
    "    F = lambda x, F_prev=F, h=h_t: F_prev(x) + eta * h(x)\n",
    "\n",
    "# Final model: F(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1beb48-9a64-4ee2-99ab-b1774066e9e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657ba17-fe8e-4f7e-804e-78aa47326a30",
   "metadata": {},
   "source": [
    "# 3. XGBoost – Theorie\n",
    "\n",
    "XGBoost = Gradient Boosting + Newton-Schritt + Regularisierung + effiziente Baumkonstruktion.\n",
    "\n",
    "\n",
    "# XGBoost optimiert eine Zielfunktion:\n",
    "\n",
    "$$\n",
    "\\text{Obj} = \n",
    "\\underbrace{\\sum_{i=1}^{n} \\ell(y_i, \\hat{y}_i)}_{\\text{Data Loss}}\n",
    "+ \n",
    "\\underbrace{\\sum_{t=1}^{T} \\Omega(h_t)}_{\\text{Regularisierung}}\n",
    "$$\n",
    "\n",
    "mit\n",
    "\n",
    "$$\n",
    "\\Omega(h_t) = \\gamma \\cdot B + \\frac{\\lambda}{2} \\sum_{j=1}^{J} w_j^2\n",
    "$$\n",
    "\n",
    "\n",
    "## Zweite-Ordnung-Taylor-Approximation\n",
    "\n",
    "Für jeden Punkt rechnen wir:\n",
    "\n",
    "$$\n",
    "g_i = \\frac{\\partial \\ell}{\\partial F(x_i)}, \n",
    "\\qquad\n",
    "h_i = \\frac{\\partial^2 \\ell}{\\partial F(x_i)^2}\n",
    "$$\n",
    "\n",
    "Der Baum wird gebaut, indem auf jedem möglichen Split der **Gain** berechnet wird:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \n",
    "\\frac{1}{2}\n",
    "\\left(\n",
    "\\frac{G_L^2}{H_L+\\lambda} +\n",
    "\\frac{G_R^2}{H_R+\\lambda} -\n",
    "\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\n",
    "\\right) - \\gamma\n",
    "$$\n",
    "\n",
    "mit:\n",
    "\n",
    "- $G_L = \\sum_{i\\in L} g_i$\n",
    "- $H_L = \\sum_{i\\in L} h_i$\n",
    "\n",
    "etc.\n",
    "\n",
    "\n",
    "## Optimaler Blattwert (closed form)\n",
    "\n",
    "$$\n",
    "w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "\n",
    "## Update-Regel\n",
    "\n",
    "$$\n",
    "F_t(x) = F_{t-1}(x) + \\eta \\cdot h_t(x)\n",
    "$$\n",
    "\n",
    "\n",
    "# Was unterscheidet XGBoost?\n",
    "\n",
    "1. Verwendet **zweite Ableitungen** → Newton-Schritt  \n",
    "2. Explizite **Regularisierung** pro Baum und Blatt  \n",
    "3. Sehr effiziente Baum-Split-Berechnung  \n",
    "4. Ausgefeilte Heuristiken (subsample, colsample, binning)  \n",
    "5. De facto **State of the Art** bei strukturierten Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ae18f-ad0c-4ca9-9f3a-e48d92a2f5f5",
   "metadata": {},
   "source": [
    "# High-level pseudocode for XGBoost (simplified for teaching)\n",
    "\n",
    "initialize F = constant_prediction()\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    \n",
    "    # 1. First and second derivatives of loss\n",
    "    g = [grad_loss(y[i], F(x_i)) for i in range(n)]\n",
    "    h = [hess_loss(y[i], F(x_i)) for i in range(n)]\n",
    "    \n",
    "    # 2. Build tree maximizing gain using g and h\n",
    "    tree = build_tree_with_gain(g, h, params)\n",
    "    \n",
    "    # 3. Compute optimal leaf weights\n",
    "    for leaf in tree.leaves:\n",
    "        G = sum(g[i] for i in leaf.samples)\n",
    "        H = sum(h[i] for i in leaf.samples)\n",
    "        leaf.weight = - G / (H + lambda_reg)\n",
    "    \n",
    "    # 4. Update model\n",
    "    F = lambda x, F_prev=F, tr=tree: F_prev(x) + eta * tr(x)\n",
    "\n",
    "# Final strong model F(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f643c-ed44-4aab-878b-6ebc8f76c668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9d22e-9093-4599-9dcc-653d15569844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730657e7-21f7-46f9-8de9-246f7a978400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11bf8a-c1fe-4d1d-9bc1-61a5fb129b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450e3e-9d0c-40dc-aa4c-a307a48405dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c19cb-7e28-4c99-ba09-fd8084cc9fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124b474-c140-416a-9b69-955d0a0625fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml25)",
   "language": "python",
   "name": "aml25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
